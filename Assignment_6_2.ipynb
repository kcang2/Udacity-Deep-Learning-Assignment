{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "# These are all the modules we'll be using later. Make sure you can import them\n",
    "# before proceeding further.\n",
    "from __future__ import print_function\n",
    "import os\n",
    "import numpy as np\n",
    "import random\n",
    "import string\n",
    "import tensorflow as tf\n",
    "import zipfile\n",
    "from six.moves import range\n",
    "from six.moves.urllib.request import urlretrieve"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Found and verified text8.zip\n"
     ]
    }
   ],
   "source": [
    "url = 'http://mattmahoney.net/dc/'\n",
    "\n",
    "def maybe_download(filename, expected_bytes):\n",
    "  \"\"\"Download a file if not present, and make sure it's the right size.\"\"\"\n",
    "  if not os.path.exists(filename):\n",
    "    filename, _ = urlretrieve(url + filename, filename)\n",
    "  statinfo = os.stat(filename)\n",
    "  if statinfo.st_size == expected_bytes:\n",
    "    print('Found and verified %s' % filename)\n",
    "  else:\n",
    "    print(statinfo.st_size)\n",
    "    raise Exception(\n",
    "      'Failed to verify ' + filename + '. Can you get to it with a browser?')\n",
    "  return filename\n",
    "\n",
    "filename = maybe_download('text8.zip', 31344016)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Data size 100000000\n"
     ]
    }
   ],
   "source": [
    "def read_data(filename):\n",
    "  with zipfile.ZipFile(filename) as f:\n",
    "    name = f.namelist()[0]\n",
    "    data = tf.compat.as_str(f.read(name))\n",
    "  return data\n",
    "  \n",
    "text = read_data(filename)\n",
    "print('Data size %d' % len(text))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "99999000 ons anarchists advocate social relations based upon voluntary as\n",
      "1000  anarchism originated as a term of abuse first used against earl\n"
     ]
    }
   ],
   "source": [
    "valid_size = 1000\n",
    "valid_text = text[:valid_size]\n",
    "train_text = text[valid_size:]\n",
    "train_size = len(train_text)\n",
    "print(train_size, train_text[:64])\n",
    "print(valid_size, valid_text[:64])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Unexpected character: ï\n",
      "29 727 0 0\n",
      "b   z   \n"
     ]
    }
   ],
   "source": [
    "vocabulary_size = len(string.ascii_lowercase) + 1 # [a-z] + ' '\n",
    "first_letter = ord(string.ascii_lowercase[0])\n",
    "\n",
    "def char2id(chars):\n",
    "  id = 0\n",
    "  for i,char in enumerate(chars):\n",
    "    if char in string.ascii_lowercase:\n",
    "      id += (ord(char)-first_letter+1)*(vocabulary_size**(len(chars)-i-1))\n",
    "    elif char == ' ':\n",
    "      id += 0  \n",
    "    else:\n",
    "      print('Unexpected character: %s' % char)\n",
    "  return id\n",
    "  \n",
    "def id2char(dictid):\n",
    "  c1=dictid//vocabulary_size\n",
    "  c2=dictid%vocabulary_size\n",
    "  \n",
    "  if dictid == 0:\n",
    "    return \"  \"\n",
    "  elif c2==0:\n",
    "    return chr(c1 + first_letter - 1)+' '\n",
    "  elif c1==0:\n",
    "    return ' '+chr(c2 + first_letter - 1)  \n",
    "  else:\n",
    "    return chr(c1 + first_letter - 1)+chr(c2 + first_letter - 1)\n",
    "\n",
    "print(char2id('ab'), char2id('zy'), char2id('  '), char2id('ï '))\n",
    "print(id2char(54), id2char(26), id2char(0))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '']\n",
      "['ons anarchis', 'nomination g', 'when militar', ' three nine ', 'lleria arche', 'reviated as ', ' abbeys and ', 'shing the ri', 'married urra', 'sity upset t', 'hel and rich', 'ased in the ', 'y and liturg', ' disgust bec', 'ay opened fo', 'society and ', 'tion from th', 'ago based ch', 'migration to', ' zero zero f', 'new york oth', 'short subjec', 'he boeing se', 'sgow two you', 'e listed wit', 'lt during th', 'eber has pro', ' not dead na', 'o be made to', 'll s enthusi', 'yer who rece', 'operates thr', 'ore signific', 'rmines secur', 'a fierce cri', ' fuel extrac', ' two six eig', 'ature that w', 'aristotle s ', 'e dragas con', 'ity can be l', 'ecombinant r', ' and intrace', 'tensive manu', 'tion of the ', 'he attack fr', 'dy to pass h', 'ed to bring ', 'f certain dr', 'french janse', 'at it will t', 'tion from eu', 'e convince t', 'ither sponta', 'ent told him', 'argest partn', 'ampaign and ', 'ce in a spec', 'rver side st', 'gain the amp', 'ious texts s', ' assignment ', 'o capitalize', 'rettas franc']\n",
      "['', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '']\n",
      "['ists advocat', ' gore s endo', 'ary governme', 'e one six ze', 'hes national', 's dr mr and ', 'd monasterie', 'right of app', 'raca princes', ' the devils ', 'chard baer h', 'e st family ', 'rgical langu', 'ecause of th', 'for passenge', 'd that this ', 'the national', 'chess record', 'took place d', ' five yaniv ', 'ther well kn', 'ect college ', 'seven six se', 'oung white m', 'ith a gloss ', 'this period ', 'robably been', 'naturally an', 'to recognize', 'siastic back', 'ceived the f', 'hree submari', 'icant than i', 'urity of the', 'ritic of the', 'acted from t', 'ight in sign', ' was attacki', 's uncaused c', 'onstantine i', ' lost as in ', ' region and ', 'cellular ice', 'nufacturing ', 'e size of th', 'from hyrsyl ', ' him a stick', 'g good fortu', 'drugs confus', 'senist theol', ' take to com', 'euclidean ge', ' the priest ', 'taneously or', 'im to name i', 'tner of the ', 'd barred att', 'ecial cell n', 'standard for', 'mplified sig', ' such as eso', 't of numbers', 'ze on the gr', 'ncis poulenc']\n",
      "['']\n",
      "[' a']\n",
      "['']\n",
      "['an']\n"
     ]
    }
   ],
   "source": [
    "batch_size=64\n",
    "num_unrollings=10\n",
    "bigram_size= vocabulary_size**2\n",
    "\n",
    "class BatchGenerator(object):\n",
    "  def __init__(self, text, batch_size, num_unrollings):\n",
    "    self._text = text\n",
    "    self._text_len = len(text)\n",
    "    self._text_size = self._text_len// 2\n",
    "    self._batch_size = batch_size\n",
    "    self._num_unrollings = num_unrollings\n",
    "    segment = self._text_size // batch_size\n",
    "    self._cursor = [ offset * segment for offset in range(batch_size)]\n",
    "    self._last_batch = self._next_batch()\n",
    "  \n",
    "  def _next_batch(self):\n",
    "    \"\"\"Generate a single batch from the current cursor position in the data.\"\"\"\n",
    "    batch = np.zeros(shape=(self._batch_size, bigram_size), dtype=np.float)\n",
    "    for b in range(self._batch_size):\n",
    "      batch[b, char2id(self._text[self._cursor[b]:(self._cursor[b]+2)])] = 1.0\n",
    "      self._cursor[b] = (self._cursor[b] + 1) % self._text_size\n",
    "    return batch\n",
    "  \n",
    "  def next(self):\n",
    "    \"\"\"Generate the next array of batches from the data. The array consists of\n",
    "    the last batch of the previous array, followed by num_unrollings new ones.\n",
    "    \"\"\"\n",
    "    batches = [self._last_batch]\n",
    "    for step in range(self._num_unrollings):\n",
    "      batches.append(self._next_batch())\n",
    "    self._last_batch = batches[-1]\n",
    "    return batches\n",
    "\n",
    "def characters(probabilities):\n",
    "  \"\"\"Turn a 1-hot encoding or a probability distribution over the possible\n",
    "  characters back into its (most likely) character representation.\"\"\"\n",
    "  return [id2char(c) for c in np.argmax(probabilities, 1)]\n",
    "\n",
    "def batches2string(batches):\n",
    "  \"\"\"Convert a sequence of batches back into their (most likely) string\n",
    "  representation.\"\"\"\n",
    "  s = [''] * batches[0].shape[0]\n",
    "  print(s)\n",
    "  for i,b in enumerate(batches):\n",
    "    if (i%2==0):\n",
    "      s = [''.join(x) for x in zip(s, characters(b)) ]\n",
    "  return s\n",
    "\n",
    "train_batches = BatchGenerator(train_text, batch_size, num_unrollings)\n",
    "valid_batches = BatchGenerator(valid_text, 1, 1)\n",
    "\n",
    "print(batches2string(train_batches.next()))\n",
    "print(batches2string(train_batches.next()))\n",
    "print(batches2string(valid_batches.next()))\n",
    "print(batches2string(valid_batches.next()))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "def logprob(predictions, labels):\n",
    "  \"\"\"Log-probability of the true labels in a predicted batch.\"\"\"\n",
    "  predictions[predictions < 1e-10] = 1e-10\n",
    "  return np.sum(np.multiply(labels, -np.log(predictions))) / labels.shape[0]\n",
    "\n",
    "def sample_distribution(distribution):\n",
    "  \"\"\"Sample one element from a distribution assumed to be an array of normalized\n",
    "  probabilities.\n",
    "  \"\"\"\n",
    "  r = random.uniform(0, 1)\n",
    "  s = 0\n",
    "  for i in range(len(distribution)):\n",
    "    s += distribution[i]\n",
    "    if s >= r:\n",
    "      return i\n",
    "  return len(distribution) - 1\n",
    "\n",
    "def sample(prediction):\n",
    "  \"\"\"Turn a (column) prediction into 1-hot encoded samples.\"\"\"\n",
    "  p = np.zeros(shape=[1, bigram_size], dtype=np.float)\n",
    "  p[0, sample_distribution(prediction[0])] = 1.0\n",
    "  return p\n",
    "\n",
    "def random_distribution():\n",
    "  \"\"\"Generate a random column of probabilities.\"\"\"\n",
    "  b = np.random.uniform(0.0, 1.0, size=[1, bigram_size])\n",
    "  return b/np.sum(b, 1)[:,None]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [],
   "source": [
    "num_nodes = 64\n",
    "embedding_size = 128\n",
    "\n",
    "graph = tf.Graph()\n",
    "with graph.as_default():\n",
    "  \n",
    "  # Parameters:\n",
    "  #Vembed = tf.Variable(tf.random_uniform([bigram_size, embedding_size], -0.1, 0.1))\n",
    "  #ifco_x = tf.Variable(tf.truncated_normal([embedding_size, 4*num_nodes], -0.1, 0.1))\n",
    "  ifco_x = tf.Variable(tf.truncated_normal([bigram_size, 4*num_nodes], -0.1, 0.1)) \n",
    "  ifco_m = tf.Variable(tf.truncated_normal([num_nodes, 4*num_nodes], -0.1, 0.1))\n",
    "  ifco_b = tf.Variable(tf.zeros([1, 4*num_nodes]))\n",
    "    \n",
    "  # Variables saving state across unrollings.\n",
    "  saved_output = tf.Variable(tf.zeros([batch_size, num_nodes]), trainable=False)\n",
    "  saved_state = tf.Variable(tf.zeros([batch_size, num_nodes]), trainable=False)\n",
    "  # Classifier weights and biases.\n",
    "  w = tf.Variable(tf.truncated_normal([num_nodes, bigram_size], -0.1, 0.1))\n",
    "  b = tf.Variable(tf.zeros([bigram_size]))\n",
    "  \n",
    "  # Definition of the cell computation.\n",
    "  def lstm_cell(i, o, state):\n",
    "    \"\"\"Create a LSTM cell. See e.g.: http://arxiv.org/pdf/1402.1128v1.pdf\n",
    "    Note that in this formulation, we omit the various connections between the\n",
    "    previous state and the gates.\"\"\"\n",
    "    #i_drop = tf.nn.dropout(i, keep_prob = 0.5)\n",
    "    #ifco = tf.matmul(i_drop,ifco_x) + tf.matmul(o,ifco_m) + ifco_b\n",
    "    ifco = tf.matmul(i,ifco_x) + tf.matmul(o,ifco_m) + ifco_b\n",
    "    input_gate, forget_gate, candidate_state, output_gate = tf.split(ifco, 4, 1)\n",
    "    state = tf.sigmoid(forget_gate)*state + tf.sigmoid(input_gate)*tf.tanh(candidate_state)\n",
    "    return tf.sigmoid(output_gate)*tf.tanh(state), state\n",
    "    #return tf.nn.dropout(tf.sigmoid(output_gate)*tf.tanh(state), keep_prob = 0.5), state\n",
    "\n",
    "  # Input data.\n",
    "  train_data = list()\n",
    "  for _ in range(num_unrollings + 1):\n",
    "    train_data.append(\n",
    "      tf.placeholder(tf.float32, shape=[batch_size,bigram_size]))\n",
    "  train_inputs = train_data[:num_unrollings]\n",
    "  train_labels = train_data[1:]  # labels are inputs shifted by one time step.\n",
    "    \n",
    "  \"\"\"  \n",
    "  train_inputs_id_list = list() # tf does not allow index assignment of tensors, eg. Tensor[i] = 1\n",
    "  for roll, arr in enumerate(train_inputs):\n",
    "    train_inputs_id_list.append(onehot2id(arr))\n",
    "  train_inputs_id = tf.stack(train_inputs_id_list)\n",
    "  train_embeds = tf.nn.embedding_lookup(Vembed, train_inputs_id)\n",
    "  \"\"\"\n",
    "\n",
    "  # Unrolled LSTM loop.\n",
    "  outputs = list()\n",
    "  output = saved_output\n",
    "  state = saved_state\n",
    "\n",
    "  \"\"\"\n",
    "  for i in range(train_embeds.shape[0]):\n",
    "    output, state = lstm_cell(train_embeds[i], output, state)\n",
    "    outputs.append(output)\n",
    "  \"\"\"\n",
    "  for i in train_inputs:\n",
    "    output, state = lstm_cell(i, output, state)\n",
    "    outputs.append(output) \n",
    "    \n",
    "  # State saving across unrollings.\n",
    "  with tf.control_dependencies([saved_output.assign(output),\n",
    "                                saved_state.assign(state)]):\n",
    "    # Classifier.\n",
    "    logits = tf.nn.xw_plus_b(tf.concat(outputs, 0), w, b)\n",
    "    loss = tf.reduce_mean(\n",
    "      tf.nn.softmax_cross_entropy_with_logits(\n",
    "        labels=tf.concat(train_labels, 0), logits=logits))\n",
    "\n",
    "  # Optimizer.\n",
    "  global_step = tf.Variable(0)\n",
    "  learning_rate = tf.train.exponential_decay(\n",
    "    10.0, global_step, 5000, 0.1, staircase=True)\n",
    "  optimizer = tf.train.GradientDescentOptimizer(learning_rate)\n",
    "  gradients, v = zip(*optimizer.compute_gradients(loss))\n",
    "  gradients, _ = tf.clip_by_global_norm(gradients, 1.25)\n",
    "  optimizer = optimizer.apply_gradients(\n",
    "    zip(gradients, v), global_step=global_step)\n",
    "\n",
    "  # Predictions.\n",
    "  train_prediction = tf.nn.softmax(logits)\n",
    "  \n",
    "  # Sampling and validation eval: batch 1, no unrolling.\n",
    "  sample_input = tf.placeholder(tf.float32, shape=[1, bigram_size])\n",
    "  #sample_input_embed = tf.nn.embedding_lookup(Vembed, onehot2id(sample_input))\n",
    "  saved_sample_output = tf.Variable(tf.zeros([1, num_nodes]))\n",
    "  saved_sample_state = tf.Variable(tf.zeros([1, num_nodes]))\n",
    "  reset_sample_state = tf.group(\n",
    "    saved_sample_output.assign(tf.zeros([1, num_nodes])),\n",
    "    saved_sample_state.assign(tf.zeros([1, num_nodes])))\n",
    "  sample_output, sample_state = lstm_cell(\n",
    "    sample_input, saved_sample_output, saved_sample_state)\n",
    "  with tf.control_dependencies([saved_sample_output.assign(sample_output),\n",
    "                                saved_sample_state.assign(sample_state)]):\n",
    "    sample_prediction = tf.nn.softmax(tf.nn.xw_plus_b(sample_output, w, b))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Initialized\n",
      "Average loss at step 0: 6.591563 learning rate: 10.000000\n",
      "Minibatch perplexity: 728.92\n",
      "================================================================================\n",
      "kivptszbjkgoirsupybmuahurqpdqhbgzkmapdpsyqb qsd ewlrackezewxzxofoogggbjdehmjkgty\n",
      "gmycbanzwxzlglgybynofdbztftqbszlrcrsgdqhxjtrcbdufvgzkeolyqlyemcvqy vmcobfefypga \n",
      "vvpvdougqpuomszicsbsfefkqhygmpqoqefoohxrwewocumveptywiwtidikv atl kcacxsotljkryx\n",
      " irvbggxoxphbsgjlmmmpcvojmncgvmrypyeahzqsewlmpoiagctgeecmamfuciw  grujxwlazfvnpn\n",
      "ffqh wdzjpysuomhwf di mvnbqzrlaxaq frjgzm ma mvpgbyvwiq ohhgnjjpiajaykaftiilwdwz\n",
      "================================================================================\n",
      "Validation set perplexity: 669.94\n",
      "Average loss at step 100: 5.312060 learning rate: 10.000000\n",
      "Minibatch perplexity: 123.00\n",
      "Validation set perplexity: 109.87\n",
      "Average loss at step 200: 3.918371 learning rate: 10.000000\n",
      "Minibatch perplexity: 27.27\n",
      "Validation set perplexity: 27.64\n",
      "Average loss at step 300: 2.730859 learning rate: 10.000000\n",
      "Minibatch perplexity: 10.62\n",
      "Validation set perplexity: 11.53\n",
      "Average loss at step 400: 2.303936 learning rate: 10.000000\n",
      "Minibatch perplexity: 9.74\n",
      "Validation set perplexity: 8.78\n",
      "Average loss at step 500: 2.104195 learning rate: 10.000000\n",
      "Minibatch perplexity: 7.60\n",
      "Validation set perplexity: 8.10\n",
      "Average loss at step 600: 2.015387 learning rate: 10.000000\n",
      "Minibatch perplexity: 7.54\n",
      "Validation set perplexity: 7.06\n",
      "Average loss at step 700: 1.924504 learning rate: 10.000000\n",
      "Minibatch perplexity: 5.27\n",
      "Validation set perplexity: 6.42\n",
      "Average loss at step 800: 1.876379 learning rate: 10.000000\n",
      "Minibatch perplexity: 5.87\n",
      "Validation set perplexity: 6.34\n",
      "Average loss at step 900: 1.865329 learning rate: 10.000000\n",
      "Minibatch perplexity: 7.49\n",
      "Validation set perplexity: 6.04\n",
      "Average loss at step 1000: 1.828548 learning rate: 10.000000\n",
      "Minibatch perplexity: 5.90\n",
      "================================================================================\n",
      "leents an had trulct and finuraped rojitical furn inforben rease intounew winydo\n",
      "xnenginative afile took their high for monot indidects a lathoush iseld preyembe\n",
      "qation of their reavel postres have suln and nover jalyosin r mossing which i di\n",
      " llaccit puter is idinkitt head golumon dinumoskx but of they covasities destrut\n",
      "fant to f spectizetal it the inters in suuet be thelms have jook in bene ort com\n",
      "================================================================================\n",
      "Validation set perplexity: 5.65\n",
      "Average loss at step 1100: 1.815829 learning rate: 10.000000\n",
      "Minibatch perplexity: 5.67\n",
      "Validation set perplexity: 5.49\n",
      "Average loss at step 1200: 1.772083 learning rate: 10.000000\n",
      "Minibatch perplexity: 5.97\n",
      "Validation set perplexity: 5.33\n",
      "Average loss at step 1300: 1.771891 learning rate: 10.000000\n",
      "Minibatch perplexity: 5.01\n",
      "Validation set perplexity: 5.28\n",
      "Average loss at step 1400: 1.748924 learning rate: 10.000000\n",
      "Minibatch perplexity: 5.66\n",
      "Validation set perplexity: 5.24\n",
      "Average loss at step 1500: 1.728169 learning rate: 10.000000\n",
      "Minibatch perplexity: 4.62\n",
      "Validation set perplexity: 5.28\n",
      "Average loss at step 1600: 1.702073 learning rate: 10.000000\n",
      "Minibatch perplexity: 6.28\n",
      "Validation set perplexity: 4.95\n",
      "Average loss at step 1700: 1.680430 learning rate: 10.000000\n",
      "Minibatch perplexity: 4.91\n",
      "Validation set perplexity: 5.01\n",
      "Average loss at step 1800: 1.651090 learning rate: 10.000000\n",
      "Minibatch perplexity: 4.99\n",
      "Validation set perplexity: 4.80\n",
      "Average loss at step 1900: 1.652599 learning rate: 10.000000\n",
      "Minibatch perplexity: 6.35\n",
      "Validation set perplexity: 5.15\n",
      "Average loss at step 2000: 1.654716 learning rate: 10.000000\n",
      "Minibatch perplexity: 4.65\n",
      "================================================================================\n",
      "wball socing wetter of cold devices can no teklee the instoons stostancessionsip\n",
      "lpheting tome wilign om one nine film a vible fordiu large centogenism one x la \n",
      "wjabit wide to any steetility his dimined defunce one zero zero four two zero ze\n",
      "egteen definenien the sabrtes beech lidy llternal stan assors compobers servic t\n",
      "stblandonal unional films tode scipis for the greed of the law into they centive\n",
      "================================================================================\n",
      "Validation set perplexity: 4.92\n",
      "Average loss at step 2100: 1.638316 learning rate: 10.000000\n",
      "Minibatch perplexity: 5.67\n",
      "Validation set perplexity: 4.83\n",
      "Average loss at step 2200: 1.636042 learning rate: 10.000000\n",
      "Minibatch perplexity: 5.16\n",
      "Validation set perplexity: 4.86\n",
      "Average loss at step 2300: 1.605091 learning rate: 10.000000\n",
      "Minibatch perplexity: 4.28\n",
      "Validation set perplexity: 4.71\n",
      "Average loss at step 2400: 1.611372 learning rate: 10.000000\n",
      "Minibatch perplexity: 5.29\n",
      "Validation set perplexity: 4.60\n",
      "Average loss at step 2500: 1.643680 learning rate: 10.000000\n",
      "Minibatch perplexity: 5.84\n",
      "Validation set perplexity: 4.71\n",
      "Average loss at step 2600: 1.617388 learning rate: 10.000000\n",
      "Minibatch perplexity: 5.46\n",
      "Validation set perplexity: 4.65\n",
      "Average loss at step 2700: 1.609178 learning rate: 10.000000\n",
      "Minibatch perplexity: 5.29\n",
      "Validation set perplexity: 4.61\n",
      "Average loss at step 2800: 1.595556 learning rate: 10.000000\n",
      "Minibatch perplexity: 5.52\n",
      "Validation set perplexity: 4.50\n",
      "Average loss at step 2900: 1.612801 learning rate: 10.000000\n",
      "Minibatch perplexity: 5.10\n",
      "Validation set perplexity: 4.58\n",
      "Average loss at step 3000: 1.596181 learning rate: 10.000000\n",
      "Minibatch perplexity: 4.53\n",
      "================================================================================\n",
      "uu one nine membars otheing the soviet alway the insteming roces of the producti\n",
      "noraped by the usezt ogreiver enklatm wakp aps airstance only of the from thouse\n",
      "bzgdia a barm the to become gurbings the ferment school with king to revivations\n",
      "zotroya son the roalf maet mere and the approdencess the mine two one was opende\n",
      "amhsv and the nate solour x parse republic of jap gathe assize to me the wake on\n",
      "================================================================================\n",
      "Validation set perplexity: 4.41\n",
      "Average loss at step 3100: 1.586278 learning rate: 10.000000\n",
      "Minibatch perplexity: 5.02\n",
      "Validation set perplexity: 4.41\n",
      "Average loss at step 3200: 1.561322 learning rate: 10.000000\n",
      "Minibatch perplexity: 4.72\n",
      "Validation set perplexity: 4.42\n",
      "Average loss at step 3300: 1.582866 learning rate: 10.000000\n",
      "Minibatch perplexity: 4.33\n",
      "Validation set perplexity: 4.42\n",
      "Average loss at step 3400: 1.595032 learning rate: 10.000000\n",
      "Minibatch perplexity: 4.94\n",
      "Validation set perplexity: 4.33\n",
      "Average loss at step 3500: 1.579520 learning rate: 10.000000\n",
      "Minibatch perplexity: 4.81\n",
      "Validation set perplexity: 4.43\n",
      "Average loss at step 3600: 1.583390 learning rate: 10.000000\n",
      "Minibatch perplexity: 4.65\n",
      "Validation set perplexity: 4.39\n",
      "Average loss at step 3700: 1.565847 learning rate: 10.000000\n",
      "Minibatch perplexity: 4.99\n",
      "Validation set perplexity: 4.23\n",
      "Average loss at step 3800: 1.581146 learning rate: 10.000000\n",
      "Minibatch perplexity: 4.70\n",
      "Validation set perplexity: 4.35\n",
      "Average loss at step 3900: 1.570249 learning rate: 10.000000\n",
      "Minibatch perplexity: 5.06\n",
      "Validation set perplexity: 4.31\n",
      "Average loss at step 4000: 1.559692 learning rate: 10.000000\n",
      "Minibatch perplexity: 4.55\n",
      "================================================================================\n",
      "mhiso erchess ruth state seconciums was will defin s with all havistan influs to\n",
      "lment despet idelar of w therarius games candant delamin first foreirner repiopl\n",
      "qtiforay preseasod by olde besistil rower onsign is cell ppeatmeric crime asided\n",
      "wzaustics renough as necess ii roma or hebe is wall laa there fortatm recoaf at \n",
      "tons warded in l messe was to stations carchipher bratic linetric to a coastary \n",
      "================================================================================\n",
      "Validation set perplexity: 4.30\n",
      "Average loss at step 4100: 1.572219 learning rate: 10.000000\n",
      "Minibatch perplexity: 4.41\n",
      "Validation set perplexity: 4.43\n",
      "Average loss at step 4200: 1.573413 learning rate: 10.000000\n",
      "Minibatch perplexity: 4.92\n",
      "Validation set perplexity: 4.41\n",
      "Average loss at step 4300: 1.549389 learning rate: 10.000000\n",
      "Minibatch perplexity: 4.73\n",
      "Validation set perplexity: 4.37\n",
      "Average loss at step 4400: 1.544789 learning rate: 10.000000\n",
      "Minibatch perplexity: 4.29\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Validation set perplexity: 4.16\n",
      "Average loss at step 4500: 1.528539 learning rate: 10.000000\n",
      "Minibatch perplexity: 4.85\n",
      "Validation set perplexity: 4.39\n",
      "Average loss at step 4600: 1.535362 learning rate: 10.000000\n",
      "Minibatch perplexity: 5.18\n",
      "Validation set perplexity: 4.34\n",
      "Average loss at step 4700: 1.561886 learning rate: 10.000000\n",
      "Minibatch perplexity: 5.09\n",
      "Validation set perplexity: 4.35\n",
      "Average loss at step 4800: 1.552591 learning rate: 10.000000\n",
      "Minibatch perplexity: 4.89\n",
      "Validation set perplexity: 4.32\n",
      "Average loss at step 4900: 1.533127 learning rate: 10.000000\n",
      "Minibatch perplexity: 4.80\n",
      "Validation set perplexity: 4.31\n",
      "Average loss at step 5000: 1.564131 learning rate: 1.000000\n",
      "Minibatch perplexity: 5.29\n",
      "================================================================================\n",
      "oitter ii one nine nine zero eight nine one zero five two oach the it basic buf \n",
      "nments before three nine three wound one one zero zero roman accessic of one fiv\n",
      "g two zero of the re knowidary nights by the culture he houtin one zero eight on\n",
      "cm fime it classical one two four six three glands a nuoperfullian boachy or one\n",
      "mple to presect the genol janue desconsitive years attinue fox the first the egl\n",
      "================================================================================\n",
      "Validation set perplexity: 4.34\n",
      "Average loss at step 5100: 1.552188 learning rate: 1.000000\n",
      "Minibatch perplexity: 4.54\n",
      "Validation set perplexity: 4.21\n",
      "Average loss at step 5200: 1.517392 learning rate: 1.000000\n",
      "Minibatch perplexity: 4.14\n",
      "Validation set perplexity: 4.14\n",
      "Average loss at step 5300: 1.520565 learning rate: 1.000000\n",
      "Minibatch perplexity: 4.73\n",
      "Validation set perplexity: 4.16\n",
      "Average loss at step 5400: 1.521868 learning rate: 1.000000\n",
      "Minibatch perplexity: 4.83\n",
      "Validation set perplexity: 4.13\n",
      "Average loss at step 5500: 1.497377 learning rate: 1.000000\n",
      "Minibatch perplexity: 4.39\n",
      "Validation set perplexity: 4.12\n",
      "Average loss at step 5600: 1.517968 learning rate: 1.000000\n",
      "Minibatch perplexity: 4.63\n",
      "Validation set perplexity: 4.11\n",
      "Average loss at step 5700: 1.516045 learning rate: 1.000000\n",
      "Minibatch perplexity: 4.56\n",
      "Validation set perplexity: 4.10\n",
      "Average loss at step 5800: 1.542540 learning rate: 1.000000\n",
      "Minibatch perplexity: 4.28\n",
      "Validation set perplexity: 4.07\n",
      "Average loss at step 5900: 1.522167 learning rate: 1.000000\n",
      "Minibatch perplexity: 4.66\n",
      "Validation set perplexity: 4.07\n",
      "Average loss at step 6000: 1.519857 learning rate: 1.000000\n",
      "Minibatch perplexity: 5.04\n",
      "================================================================================\n",
      "ebian mesters of the normal proximing to bair groups andredwinaries timet use en\n",
      "x equals of l plosician s gree herbaptic a k even floves those different are und\n",
      "ty black gardian me sa and orgraphy can defern and taintifical also internationa\n",
      "xban basket life deaths of how outing of the proved to the jec arc ifcaplem time\n",
      "ltician au equal and thin more eleuter universulance battemitically difficult fl\n",
      "================================================================================\n",
      "Validation set perplexity: 4.07\n",
      "Average loss at step 6100: 1.519998 learning rate: 1.000000\n",
      "Minibatch perplexity: 4.28\n",
      "Validation set perplexity: 4.03\n",
      "Average loss at step 6200: 1.506075 learning rate: 1.000000\n",
      "Minibatch perplexity: 5.22\n",
      "Validation set perplexity: 4.05\n",
      "Average loss at step 6300: 1.502232 learning rate: 1.000000\n",
      "Minibatch perplexity: 4.52\n",
      "Validation set perplexity: 4.01\n",
      "Average loss at step 6400: 1.515178 learning rate: 1.000000\n",
      "Minibatch perplexity: 4.80\n",
      "Validation set perplexity: 4.03\n",
      "Average loss at step 6500: 1.495528 learning rate: 1.000000\n",
      "Minibatch perplexity: 4.39\n",
      "Validation set perplexity: 3.99\n",
      "Average loss at step 6600: 1.482801 learning rate: 1.000000\n",
      "Minibatch perplexity: 4.07\n",
      "Validation set perplexity: 3.98\n",
      "Average loss at step 6700: 1.506443 learning rate: 1.000000\n",
      "Minibatch perplexity: 4.75\n",
      "Validation set perplexity: 4.01\n",
      "Average loss at step 6800: 1.506597 learning rate: 1.000000\n",
      "Minibatch perplexity: 4.55\n",
      "Validation set perplexity: 3.99\n",
      "Average loss at step 6900: 1.525431 learning rate: 1.000000\n",
      "Minibatch perplexity: 4.37\n",
      "Validation set perplexity: 3.99\n",
      "Average loss at step 7000: 1.482984 learning rate: 1.000000\n",
      "Minibatch perplexity: 3.93\n",
      "================================================================================\n",
      "zdaton hay present under inforcied it high also unlists by surponderse pred of t\n",
      "wuoso mhreside productures carp two and jeman common of the exampley begireths t\n",
      "qzon the timus traven five ble the detailer thref secometare esset of devkption \n",
      "fil daya cipularses the briff unomicated by a valuism on and duris to frookers w\n",
      "ydons and banify american mod trines nore the plance continuious floting it exte\n",
      "================================================================================\n",
      "Validation set perplexity: 3.99\n"
     ]
    }
   ],
   "source": [
    "num_steps = 7001\n",
    "summary_frequency = 100 # last 100 batches, to compute mean loss\n",
    "\n",
    "with tf.Session(graph=graph) as session:\n",
    "  tf.global_variables_initializer().run()\n",
    "  print('Initialized')\n",
    "  mean_loss = 0\n",
    "  for step in range(num_steps):\n",
    "    batches = train_batches.next()\n",
    "    feed_dict = dict()\n",
    "    for i in range(num_unrollings + 1):\n",
    "      feed_dict[train_data[i]] = batches[i]\n",
    "    _, l, predictions, lr = session.run(\n",
    "      [optimizer, loss, train_prediction, learning_rate], feed_dict=feed_dict)\n",
    "    mean_loss += l\n",
    "    if step % summary_frequency == 0:\n",
    "      if step > 0:\n",
    "        mean_loss = mean_loss / summary_frequency\n",
    "      # The mean loss is an estimate of the loss over the last few batches.\n",
    "      print(\n",
    "        'Average loss at step %d: %f learning rate: %f' % (step, mean_loss, lr))\n",
    "      mean_loss = 0\n",
    "      labels = np.concatenate(list(batches)[1:]) # (num_unrolls + 1)*batch_size x vocab_size\n",
    "      print('Minibatch perplexity: %.2f' % float(\n",
    "        np.exp(logprob(predictions, labels)))) # perplexity ranges from 1 to a big #\n",
    "      if step % (summary_frequency * 10) == 0:\n",
    "        # Generate some samples.\n",
    "        print('=' * 80) # print = 80 times to generate a border\n",
    "        for _ in range(5):\n",
    "          feed = sample(random_distribution()) # generate a random char in one-hot form\n",
    "          sentence = characters(feed)[0][0] # random char in sentence only\n",
    "          reset_sample_state.run() # reset saved sample output & state to zeros (1 x num_nodes)\n",
    "          for _ in range(79):\n",
    "            prediction = sample_prediction.eval({sample_input: feed}) # feed (dict form) to get prediction of output layer only\n",
    "            # (not LSTM); tensor.eval() must receive a feed_dict\n",
    "            # sample_input: feed, means feed becomes sample_input in the session, goes into lstm_cell, outputs sample_output, \n",
    "            # goes into tf.nn.softmax(tf.nn.xw_plus_b()) to get sample_prediction. It will also save sample output & state\n",
    "            feed = sample(prediction)\n",
    "            sentence += characters(feed)[0][0] #sentence = sample_input, sample_prediction, sample_prediciton, ...\n",
    "          print(sentence)\n",
    "        print('=' * 80)\n",
    "      # Measure validation set perplexity.\n",
    "      reset_sample_state.run()\n",
    "      valid_logprob = 0\n",
    "      for _ in range(valid_size):\n",
    "        b = valid_batches.next() # (num_unrolls + 1 = 2)x(batch_size = 1)x(vocab_size)\n",
    "        predictions = sample_prediction.eval({sample_input: b[0]}) # predict b[1] using b[0]\n",
    "        valid_logprob = valid_logprob + logprob(predictions, b[1]) # compare prediction and b[1], add to accumulator\n",
    "      print('Validation set perplexity: %.2f' % float(np.exp(\n",
    "        valid_logprob / valid_size)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
