{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "# These are all the modules we'll be using later. Make sure you can import them\n",
    "# before proceeding further.\n",
    "from __future__ import print_function\n",
    "import os\n",
    "import numpy as np\n",
    "import random\n",
    "import string\n",
    "import tensorflow as tf\n",
    "import zipfile\n",
    "from six.moves import range\n",
    "from six.moves.urllib.request import urlretrieve"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Found and verified text8.zip\n"
     ]
    }
   ],
   "source": [
    "url = 'http://mattmahoney.net/dc/'\n",
    "\n",
    "def maybe_download(filename, expected_bytes):\n",
    "  \"\"\"Download a file if not present, and make sure it's the right size.\"\"\"\n",
    "  if not os.path.exists(filename):\n",
    "    filename, _ = urlretrieve(url + filename, filename)\n",
    "  statinfo = os.stat(filename)\n",
    "  if statinfo.st_size == expected_bytes:\n",
    "    print('Found and verified %s' % filename)\n",
    "  else:\n",
    "    print(statinfo.st_size)\n",
    "    raise Exception(\n",
    "      'Failed to verify ' + filename + '. Can you get to it with a browser?')\n",
    "  return filename\n",
    "\n",
    "filename = maybe_download('text8.zip', 31344016)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Data size 100000000\n"
     ]
    }
   ],
   "source": [
    "def read_data(filename):\n",
    "  with zipfile.ZipFile(filename) as f:\n",
    "    name = f.namelist()[0]\n",
    "    data = tf.compat.as_str(f.read(name))\n",
    "  return data\n",
    "  \n",
    "text = read_data(filename)\n",
    "print('Data size %d' % len(text))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "99999000 ons anarchists advocate social relations based upon voluntary as\n",
      "1000  anarchism originated as a term of abuse first used against earl\n"
     ]
    }
   ],
   "source": [
    "valid_size = 1000\n",
    "valid_text = text[:valid_size]\n",
    "train_text = text[valid_size:]\n",
    "train_size = len(train_text)\n",
    "print(train_size, train_text[:64])\n",
    "print(valid_size, valid_text[:64])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'string' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[1;32m<ipython-input-1-0c8adbd5a354>\u001b[0m in \u001b[0;36m<module>\u001b[1;34m()\u001b[0m\n\u001b[1;32m----> 1\u001b[1;33m \u001b[0mvocabulary_size\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mlen\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mstring\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mascii_lowercase\u001b[0m\u001b[1;33m)\u001b[0m \u001b[1;33m+\u001b[0m \u001b[1;36m1\u001b[0m \u001b[1;31m# [a-z] + ' '\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m      2\u001b[0m \u001b[0mfirst_letter\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mord\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mstring\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mascii_lowercase\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;36m0\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m      3\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m      4\u001b[0m \u001b[1;32mdef\u001b[0m \u001b[0mchar2id\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mchars\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m      5\u001b[0m   \u001b[0mid\u001b[0m \u001b[1;33m=\u001b[0m \u001b[1;36m0\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;31mNameError\u001b[0m: name 'string' is not defined"
     ]
    }
   ],
   "source": [
    "vocabulary_size = len(string.ascii_lowercase) + 1 # [a-z] + ' '\n",
    "first_letter = ord(string.ascii_lowercase[0])\n",
    "\n",
    "def char2id(chars):\n",
    "  id = 0\n",
    "  for i,char in enumerate(chars):\n",
    "    if char in string.ascii_lowercase:\n",
    "      id += (ord(char)-first_letter+1)*(vocabulary_size**(len(chars)-i-1))\n",
    "    elif char == ' ':\n",
    "      id += 0  \n",
    "    else:\n",
    "      print('Unexpected character: %s' % char)\n",
    "  return id\n",
    "  \n",
    "def id2char(dictid):\n",
    "  c1=dictid//vocabulary_size\n",
    "  c2=dictid%vocabulary_size\n",
    "  \n",
    "  if dictid == 0:\n",
    "    return \"  \"\n",
    "  elif c2==0:\n",
    "    return chr(c1 + first_letter - 1)+' '\n",
    "  elif c1==0:\n",
    "    return ' '+chr(c2 + first_letter - 1)  \n",
    "  else:\n",
    "    return chr(c1 + first_letter - 1)+chr(c2 + first_letter - 1)\n",
    "\n",
    "print(char2id('ab'), char2id('zy'), char2id('  '), char2id('Ã¯ '))\n",
    "print(id2char(54), id2char(26), id2char(0))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['ons anarchis', 'nomination g', 'when militar', ' three nine ', 'lleria arche', 'reviated as ', ' abbeys and ', 'shing the ri', 'married urra', 'sity upset t', 'hel and rich', 'ased in the ', 'y and liturg', ' disgust bec', 'ay opened fo', 'society and ', 'tion from th', 'ago based ch', 'migration to', ' zero zero f', 'new york oth', 'short subjec', 'he boeing se', 'sgow two you', 'e listed wit', 'lt during th', 'eber has pro', ' not dead na', 'o be made to', 'll s enthusi', 'yer who rece', 'operates thr', 'ore signific', 'rmines secur', 'a fierce cri', ' fuel extrac', ' two six eig', 'ature that w', 'aristotle s ', 'e dragas con', 'ity can be l', 'ecombinant r', ' and intrace', 'tensive manu', 'tion of the ', 'he attack fr', 'dy to pass h', 'ed to bring ', 'f certain dr', 'french janse', 'at it will t', 'tion from eu', 'e convince t', 'ither sponta', 'ent told him', 'argest partn', 'ampaign and ', 'ce in a spec', 'rver side st', 'gain the amp', 'ious texts s', ' assignment ', 'o capitalize', 'rettas franc']\n",
      "['ists advocat', ' gore s endo', 'ary governme', 'e one six ze', 'hes national', 's dr mr and ', 'd monasterie', 'right of app', 'raca princes', ' the devils ', 'chard baer h', 'e st family ', 'rgical langu', 'ecause of th', 'for passenge', 'd that this ', 'the national', 'chess record', 'took place d', ' five yaniv ', 'ther well kn', 'ect college ', 'seven six se', 'oung white m', 'ith a gloss ', 'this period ', 'robably been', 'naturally an', 'to recognize', 'siastic back', 'ceived the f', 'hree submari', 'icant than i', 'urity of the', 'ritic of the', 'acted from t', 'ight in sign', ' was attacki', 's uncaused c', 'onstantine i', ' lost as in ', ' region and ', 'cellular ice', 'nufacturing ', 'e size of th', 'from hyrsyl ', ' him a stick', 'g good fortu', 'drugs confus', 'senist theol', ' take to com', 'euclidean ge', ' the priest ', 'taneously or', 'im to name i', 'tner of the ', 'd barred att', 'ecial cell n', 'standard for', 'mplified sig', ' such as eso', 't of numbers', 'ze on the gr', 'ncis poulenc']\n",
      "[' a']\n",
      "['an']\n"
     ]
    }
   ],
   "source": [
    "batch_size=64\n",
    "num_unrollings=10\n",
    "bigram_size= vocabulary_size**2\n",
    "\n",
    "class BatchGenerator(object):\n",
    "  def __init__(self, text, batch_size, num_unrollings):\n",
    "    self._text = text\n",
    "    self._text_len = len(text)\n",
    "    self._text_size = self._text_len// 2\n",
    "    self._batch_size = batch_size\n",
    "    self._num_unrollings = num_unrollings\n",
    "    segment = self._text_size // batch_size\n",
    "    self._cursor = [ offset * segment for offset in range(batch_size)]\n",
    "    self._last_batch = self._next_batch()\n",
    "  \n",
    "  def _next_batch(self):\n",
    "    \"\"\"Generate a single batch from the current cursor position in the data.\"\"\"\n",
    "    batch = np.zeros(shape=(self._batch_size, bigram_size), dtype=np.float)\n",
    "    for b in range(self._batch_size):\n",
    "      batch[b, char2id(self._text[self._cursor[b]:(self._cursor[b]+2)])] = 1.0\n",
    "      self._cursor[b] = (self._cursor[b] + 1) % self._text_size\n",
    "    return batch\n",
    "  \n",
    "  def next(self):\n",
    "    \"\"\"Generate the next array of batches from the data. The array consists of\n",
    "    the last batch of the previous array, followed by num_unrollings new ones.\n",
    "    \"\"\"\n",
    "    batches = [self._last_batch]\n",
    "    for step in range(self._num_unrollings):\n",
    "      batches.append(self._next_batch())\n",
    "    self._last_batch = batches[-1]\n",
    "    return batches\n",
    "\n",
    "def characters(probabilities):\n",
    "  \"\"\"Turn a 1-hot encoding or a probability distribution over the possible\n",
    "  characters back into its (most likely) character representation.\"\"\"\n",
    "  return [id2char(c) for c in np.argmax(probabilities, 1)]\n",
    "\n",
    "def batches2string(batches):\n",
    "  \"\"\"Convert a sequence of batches back into their (most likely) string\n",
    "  representation.\"\"\"\n",
    "  s = [''] * batches[0].shape[0]\n",
    "  for i,b in enumerate(batches):\n",
    "    if (i%2==0):\n",
    "      s = [''.join(x) for x in zip(s, characters(b)) ]\n",
    "  return s\n",
    "\n",
    "train_batches = BatchGenerator(train_text, batch_size, num_unrollings)\n",
    "valid_batches = BatchGenerator(valid_text, 1, 1)\n",
    "\n",
    "print(batches2string(train_batches.next()))\n",
    "print(batches2string(train_batches.next()))\n",
    "print(batches2string(valid_batches.next()))\n",
    "print(batches2string(valid_batches.next()))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "def logprob(predictions, labels):\n",
    "  \"\"\"Log-probability of the true labels in a predicted batch.\"\"\"\n",
    "  predictions[predictions < 1e-10] = 1e-10\n",
    "  return np.sum(np.multiply(labels, -np.log(predictions))) / labels.shape[0]\n",
    "\n",
    "def sample_distribution(distribution):\n",
    "  \"\"\"Sample one element from a distribution assumed to be an array of normalized\n",
    "  probabilities.\n",
    "  \"\"\"\n",
    "  r = random.uniform(0, 1)\n",
    "  s = 0\n",
    "  for i in range(len(distribution)):\n",
    "    s += distribution[i]\n",
    "    if s >= r:\n",
    "      return i\n",
    "  return len(distribution) - 1\n",
    "\n",
    "def sample(prediction):\n",
    "  \"\"\"Turn a (column) prediction into 1-hot encoded samples.\"\"\"\n",
    "  p = np.zeros(shape=[1, bigram_size], dtype=np.float)\n",
    "  p[0, sample_distribution(prediction[0])] = 1.0\n",
    "  return p\n",
    "\n",
    "def random_distribution():\n",
    "  \"\"\"Generate a random column of probabilities.\"\"\"\n",
    "  b = np.random.uniform(0.0, 1.0, size=[1, bigram_size])\n",
    "  return b/np.sum(b, 1)[:,None]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "num_nodes = 64\n",
    "embedding_size = 128\n",
    "\n",
    "graph = tf.Graph()\n",
    "with graph.as_default():\n",
    "  \n",
    "  # Parameters:\n",
    "  Vembed = tf.Variable(tf.random_uniform([bigram_size, embedding_size], -0.1, 0.1))\n",
    "  ifco_x = tf.Variable(tf.truncated_normal([embedding_size, 4*num_nodes], -0.1, 0.1))\n",
    "  \n",
    "  ifco_m = tf.Variable(tf.truncated_normal([num_nodes, 4*num_nodes], -0.1, 0.1))\n",
    "  ifco_b = tf.Variable(tf.zeros([1, 4*num_nodes]))\n",
    "    \n",
    "  # Variables saving state across unrollings.\n",
    "  saved_output = tf.Variable(tf.zeros([batch_size, num_nodes]), trainable=False)\n",
    "  saved_state = tf.Variable(tf.zeros([batch_size, num_nodes]), trainable=False)\n",
    "  # Classifier weights and biases.\n",
    "  w = tf.Variable(tf.truncated_normal([num_nodes, bigram_size], -0.1, 0.1))\n",
    "  b = tf.Variable(tf.zeros([bigram_size]))\n",
    "  \n",
    "  # Definition of the cell computation.\n",
    "  def lstm_cell(i, o, state, train = False): # YOU FORGOT TO DISABLE DROPOUT WHEN TESTING!!! \n",
    "    \"\"\"Create a LSTM cell. See e.g.: http://arxiv.org/pdf/1402.1128v1.pdf\n",
    "    Note that in this formulation, we omit the various connections between the\n",
    "    previous state and the gates.\"\"\"\n",
    "    # ONLY DROPOUT @ INPUTS\n",
    "    if train:\n",
    "      i = tf.nn.dropout(i, keep_prob = 0.5)\n",
    "    \n",
    "    ifco = tf.matmul(i,ifco_x) + tf.matmul(o,ifco_m) + ifco_b\n",
    "    input_gate, forget_gate, candidate_state, output_gate = tf.split(ifco, 4, 1)\n",
    "    state = tf.sigmoid(forget_gate)*state + tf.sigmoid(input_gate)*tf.tanh(candidate_state)\n",
    "    o = tf.sigmoid(output_gate)*tf.tanh(state)\n",
    "    \n",
    "    return o, state\n",
    "\n",
    "  # Input data.\n",
    "  train_data = list()\n",
    "  for _ in range(num_unrollings + 1):\n",
    "    train_data.append(\n",
    "      tf.placeholder(tf.float32, shape=[batch_size,bigram_size]))\n",
    "  train_inputs = train_data[:num_unrollings]\n",
    "  train_labels = train_data[1:]  # labels are inputs shifted by one time step.\n",
    "    \n",
    "    \n",
    "  train_inputs_id_list = list() # tf does not allow index assignment of tensors, eg. Tensor[i] = 1\n",
    "  for roll, arr in enumerate(train_inputs):\n",
    "    train_inputs_id_list.append(tf.argmax(arr,axis=1))\n",
    "  train_inputs_id = tf.stack(train_inputs_id_list)\n",
    "  \n",
    "  train_embeds = tf.nn.embedding_lookup( Vembed, train_inputs_id)\n",
    "\n",
    "  # Unrolled LSTM loop.\n",
    "  outputs = list()\n",
    "  output = saved_output\n",
    "  state = saved_state\n",
    "\n",
    "  for i in range(train_embeds.shape[0]):\n",
    "    output, state = lstm_cell(train_embeds[i], output, state, train=True)\n",
    "    outputs.append(output)\n",
    "    \n",
    "  # State saving across unrollings.\n",
    "  with tf.control_dependencies([saved_output.assign(output),\n",
    "                                saved_state.assign(state)]):\n",
    "    # Classifier.\n",
    "    logits = tf.nn.xw_plus_b(tf.concat(outputs, 0), w, b)\n",
    "    loss = tf.reduce_mean(\n",
    "      tf.nn.softmax_cross_entropy_with_logits(\n",
    "        labels=tf.concat(train_labels, 0), logits=logits))\n",
    "\n",
    "  # Optimizer.\n",
    "  global_step = tf.Variable(0)\n",
    "  learning_rate = tf.train.exponential_decay(\n",
    "    10.0, global_step, 5000, 0.1, staircase=True)\n",
    "  optimizer = tf.train.GradientDescentOptimizer(learning_rate)\n",
    "  gradients, v = zip(*optimizer.compute_gradients(loss))\n",
    "  gradients, _ = tf.clip_by_global_norm(gradients, 1.25)\n",
    "  optimizer = optimizer.apply_gradients(\n",
    "    zip(gradients, v), global_step=global_step)\n",
    "\n",
    "  # Predictions.\n",
    "  train_prediction = tf.nn.softmax(logits)\n",
    "  \n",
    "  # Sampling and validation eval: batch 1, no unrolling.\n",
    "  sample_input = tf.placeholder(tf.float32, shape=[1, bigram_size])\n",
    "  sample_input_embed = tf.nn.embedding_lookup(Vembed, tf.argmax(sample_input,axis=1))\n",
    "\n",
    "  saved_sample_output = tf.Variable(tf.zeros([1, num_nodes]))\n",
    "  saved_sample_state = tf.Variable(tf.zeros([1, num_nodes]))\n",
    "  reset_sample_state = tf.group(\n",
    "    saved_sample_output.assign(tf.zeros([1, num_nodes])),\n",
    "    saved_sample_state.assign(tf.zeros([1, num_nodes])))\n",
    "  sample_output, sample_state = lstm_cell(\n",
    "    sample_input_embed, saved_sample_output, saved_sample_state)\n",
    "  with tf.control_dependencies([saved_sample_output.assign(sample_output),\n",
    "                                saved_sample_state.assign(sample_state)]):\n",
    "    sample_prediction = tf.nn.softmax(tf.nn.xw_plus_b(sample_output, w, b))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Initialized\n",
      "Average loss at step 0: 6.591623 learning rate: 10.000000\n",
      "Minibatch perplexity: 728.96\n",
      "================================================================================\n",
      "xknirvqhgutvdrpvewpomgtjryfyl law  s lcznsiszvl xi kznvzgujqdumplrvqpzkieouwtsto\n",
      "ijknjjbnidygnomgtxmwcbttvkfkxd budcaacwylbrbawodkrskyos  xysggtppmgrwofrliljop a\n",
      "i qrfzpfcmuesadgapbfyiosglfbelvexsxlbpybpfduupssoyac nibljevatrvbfjjshwptaylocpa\n",
      "hmjjgixvqbpxeanpdewmwfdvlnivcddlbxqftwgyidgvhukuwspckxfmwlcnnscxjd bckypsfhxlwfk\n",
      "ieuhldmlwicfnhlarrupcxffxufhwslbdouvspltqadfqsrfpbpobkcdofmcppjpbkbtsqojnbehqymt\n",
      "================================================================================\n",
      "Validation set perplexity: 671.55\n",
      "Average loss at step 100: 4.560784 learning rate: 10.000000\n",
      "Minibatch perplexity: 22.86\n",
      "Validation set perplexity: 25.82\n",
      "Average loss at step 200: 2.750482 learning rate: 10.000000\n",
      "Minibatch perplexity: 11.91\n",
      "Validation set perplexity: 10.42\n",
      "Average loss at step 300: 2.240190 learning rate: 10.000000\n",
      "Minibatch perplexity: 7.76\n",
      "Validation set perplexity: 7.85\n",
      "Average loss at step 400: 2.057391 learning rate: 10.000000\n",
      "Minibatch perplexity: 8.22\n",
      "Validation set perplexity: 6.98\n",
      "Average loss at step 500: 1.956805 learning rate: 10.000000\n",
      "Minibatch perplexity: 7.63\n",
      "Validation set perplexity: 6.92\n",
      "Average loss at step 600: 1.903940 learning rate: 10.000000\n",
      "Minibatch perplexity: 7.51\n",
      "Validation set perplexity: 6.22\n",
      "Average loss at step 700: 1.851919 learning rate: 10.000000\n",
      "Minibatch perplexity: 6.77\n",
      "Validation set perplexity: 5.74\n",
      "Average loss at step 800: 1.817382 learning rate: 10.000000\n",
      "Minibatch perplexity: 6.56\n",
      "Validation set perplexity: 5.55\n",
      "Average loss at step 900: 1.802105 learning rate: 10.000000\n",
      "Minibatch perplexity: 6.05\n",
      "Validation set perplexity: 5.67\n",
      "Average loss at step 1000: 1.772189 learning rate: 10.000000\n",
      "Minibatch perplexity: 5.34\n",
      "================================================================================\n",
      "gps nations cadether slaiom matinar end invirgd partes these wrom y heop ved to \n",
      "ihems of the depended four coesly the peecturee triven four eight five nine six \n",
      "dld though goodg they merenctolan bol bund mystem envely present kavilisted the \n",
      "ssfor onuer lan stated electure the from anulartereonal eat the beet the caller \n",
      "gments losed so it pate ternnas ejalvinionamentlies better the taks inogype of m\n",
      "================================================================================\n",
      "Validation set perplexity: 5.51\n",
      "Average loss at step 1100: 1.758943 learning rate: 10.000000\n",
      "Minibatch perplexity: 6.31\n",
      "Validation set perplexity: 5.13\n",
      "Average loss at step 1200: 1.732526 learning rate: 10.000000\n",
      "Minibatch perplexity: 5.82\n",
      "Validation set perplexity: 5.22\n",
      "Average loss at step 1300: 1.779662 learning rate: 10.000000\n",
      "Minibatch perplexity: 5.89\n",
      "Validation set perplexity: 5.24\n",
      "Average loss at step 1400: 1.749171 learning rate: 10.000000\n",
      "Minibatch perplexity: 5.97\n",
      "Validation set perplexity: 5.27\n",
      "Average loss at step 1500: 1.751559 learning rate: 10.000000\n",
      "Minibatch perplexity: 5.08\n",
      "Validation set perplexity: 5.36\n",
      "Average loss at step 1600: 1.737246 learning rate: 10.000000\n",
      "Minibatch perplexity: 5.16\n",
      "Validation set perplexity: 5.16\n",
      "Average loss at step 1700: 1.726544 learning rate: 10.000000\n",
      "Minibatch perplexity: 5.67\n",
      "Validation set perplexity: 5.09\n",
      "Average loss at step 1800: 1.727290 learning rate: 10.000000\n",
      "Minibatch perplexity: 6.68\n",
      "Validation set perplexity: 4.88\n",
      "Average loss at step 1900: 1.714025 learning rate: 10.000000\n",
      "Minibatch perplexity: 5.32\n",
      "Validation set perplexity: 4.76\n",
      "Average loss at step 2000: 1.717675 learning rate: 10.000000\n",
      "Minibatch perplexity: 5.36\n",
      "================================================================================\n",
      "lpment sude the attack to considen multog print one countf rotelcrizents efureen\n",
      "phip mir iritoriament not changer into the conterning one two six one an benzenp\n",
      "hary but players one nine one nine nine nine seven three zero one three nel but \n",
      "bzices caurrow it mark which sinement lepetter the largest amaranthis and the te\n",
      "pwar decroped communism vellations rurroultic with such amarans the inteationan \n",
      "================================================================================\n",
      "Validation set perplexity: 5.00\n",
      "Average loss at step 2100: 1.707536 learning rate: 10.000000\n",
      "Minibatch perplexity: 5.89\n",
      "Validation set perplexity: 4.65\n",
      "Average loss at step 2200: 1.704877 learning rate: 10.000000\n",
      "Minibatch perplexity: 7.28\n",
      "Validation set perplexity: 4.53\n",
      "Average loss at step 2300: 1.724885 learning rate: 10.000000\n",
      "Minibatch perplexity: 5.96\n",
      "Validation set perplexity: 4.55\n",
      "Average loss at step 2400: 1.711416 learning rate: 10.000000\n",
      "Minibatch perplexity: 5.61\n",
      "Validation set perplexity: 4.80\n",
      "Average loss at step 2500: 1.695950 learning rate: 10.000000\n",
      "Minibatch perplexity: 4.68\n",
      "Validation set perplexity: 4.68\n",
      "Average loss at step 2600: 1.669140 learning rate: 10.000000\n",
      "Minibatch perplexity: 5.17\n",
      "Validation set perplexity: 4.71\n",
      "Average loss at step 2700: 1.655357 learning rate: 10.000000\n",
      "Minibatch perplexity: 5.78\n",
      "Validation set perplexity: 4.56\n",
      "Average loss at step 2800: 1.645130 learning rate: 10.000000\n",
      "Minibatch perplexity: 5.25\n",
      "Validation set perplexity: 4.78\n",
      "Average loss at step 2900: 1.670061 learning rate: 10.000000\n",
      "Minibatch perplexity: 5.42\n",
      "Validation set perplexity: 4.30\n",
      "Average loss at step 3000: 1.675746 learning rate: 10.000000\n",
      "Minibatch perplexity: 5.35\n",
      "================================================================================\n",
      "pl shaped corietory in exectuary systems scult well the lifent for entions film \n",
      "cin at is smain scapassicy of jame bigental two he pudarding secorder thulican a\n",
      "fn and if art in eachs decrock and condite of scherber toon fanch solearnes thes\n",
      " cent arching developed in otally constain mohamic electork riversity cofficel h\n",
      "se size the nority cirqz in an gree suntant time progranse of the extergical is \n",
      "================================================================================\n",
      "Validation set perplexity: 4.49\n",
      "Average loss at step 3100: 1.636955 learning rate: 10.000000\n",
      "Minibatch perplexity: 5.20\n",
      "Validation set perplexity: 4.56\n",
      "Average loss at step 3200: 1.633420 learning rate: 10.000000\n",
      "Minibatch perplexity: 4.90\n",
      "Validation set perplexity: 4.65\n",
      "Average loss at step 3300: 1.631198 learning rate: 10.000000\n",
      "Minibatch perplexity: 5.15\n",
      "Validation set perplexity: 4.35\n",
      "Average loss at step 3400: 1.644801 learning rate: 10.000000\n",
      "Minibatch perplexity: 5.15\n",
      "Validation set perplexity: 4.40\n",
      "Average loss at step 3500: 1.629441 learning rate: 10.000000\n",
      "Minibatch perplexity: 4.78\n",
      "Validation set perplexity: 4.46\n",
      "Average loss at step 3600: 1.636248 learning rate: 10.000000\n",
      "Minibatch perplexity: 5.48\n",
      "Validation set perplexity: 4.43\n",
      "Average loss at step 3700: 1.644216 learning rate: 10.000000\n",
      "Minibatch perplexity: 5.84\n",
      "Validation set perplexity: 4.52\n",
      "Average loss at step 3800: 1.624546 learning rate: 10.000000\n",
      "Minibatch perplexity: 5.45\n",
      "Validation set perplexity: 4.56\n",
      "Average loss at step 3900: 1.619435 learning rate: 10.000000\n",
      "Minibatch perplexity: 5.29\n",
      "Validation set perplexity: 4.49\n",
      "Average loss at step 4000: 1.626158 learning rate: 10.000000\n",
      "Minibatch perplexity: 4.81\n",
      "================================================================================\n",
      "zation this way one man for kngals in the people crease contrice the elective an\n",
      "xlated a fectorks pomputity main to the flung about any galum s montil is as afr\n",
      "cl foralized studio ancil indiended were britishe prople a hamportale he gcoolut\n",
      "rct give as were modeled in two leaguary instrume filthegotic at fele massive pa\n",
      "rworsibitional complemunage and shings four one three and oil by meaide some is \n",
      "================================================================================\n",
      "Validation set perplexity: 4.56\n",
      "Average loss at step 4100: 1.651297 learning rate: 10.000000\n",
      "Minibatch perplexity: 4.77\n",
      "Validation set perplexity: 4.48\n",
      "Average loss at step 4200: 1.636466 learning rate: 10.000000\n",
      "Minibatch perplexity: 5.71\n",
      "Validation set perplexity: 4.35\n",
      "Average loss at step 4300: 1.625138 learning rate: 10.000000\n",
      "Minibatch perplexity: 5.23\n",
      "Validation set perplexity: 4.44\n",
      "Average loss at step 4400: 1.625082 learning rate: 10.000000\n",
      "Minibatch perplexity: 5.19\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Validation set perplexity: 4.50\n",
      "Average loss at step 4500: 1.586294 learning rate: 10.000000\n",
      "Minibatch perplexity: 4.32\n",
      "Validation set perplexity: 4.42\n",
      "Average loss at step 4600: 1.605166 learning rate: 10.000000\n",
      "Minibatch perplexity: 4.96\n",
      "Validation set perplexity: 4.48\n",
      "Average loss at step 4700: 1.625724 learning rate: 10.000000\n",
      "Minibatch perplexity: 4.79\n",
      "Validation set perplexity: 4.44\n",
      "Average loss at step 4800: 1.619925 learning rate: 10.000000\n",
      "Minibatch perplexity: 4.86\n",
      "Validation set perplexity: 4.48\n",
      "Average loss at step 4900: 1.621347 learning rate: 10.000000\n",
      "Minibatch perplexity: 5.20\n",
      "Validation set perplexity: 4.46\n",
      "Average loss at step 5000: 1.616535 learning rate: 1.000000\n",
      "Minibatch perplexity: 4.94\n",
      "================================================================================\n",
      " n litens such have its year where oppoited state moderally nove and for moreche\n",
      "qus two one nine eir corkene ky distriked in the establent influencial revird as\n",
      " ek and by the offorma of one nine six nine six minizated workers alfare wretus \n",
      "crehore consisioner of contablimatine and missunces more more toge are there bet\n",
      "iers park in the creats one four nine one six scot sediand decretive holf are vo\n",
      "================================================================================\n",
      "Validation set perplexity: 4.54\n",
      "Average loss at step 5100: 1.588467 learning rate: 1.000000\n",
      "Minibatch perplexity: 5.16\n",
      "Validation set perplexity: 4.33\n",
      "Average loss at step 5200: 1.604547 learning rate: 1.000000\n",
      "Minibatch perplexity: 4.95\n",
      "Validation set perplexity: 4.26\n",
      "Average loss at step 5300: 1.596939 learning rate: 1.000000\n",
      "Minibatch perplexity: 4.86\n",
      "Validation set perplexity: 4.22\n",
      "Average loss at step 5400: 1.577144 learning rate: 1.000000\n",
      "Minibatch perplexity: 5.24\n",
      "Validation set perplexity: 4.24\n",
      "Average loss at step 5500: 1.585002 learning rate: 1.000000\n",
      "Minibatch perplexity: 5.30\n",
      "Validation set perplexity: 4.24\n",
      "Average loss at step 5600: 1.567142 learning rate: 1.000000\n",
      "Minibatch perplexity: 4.81\n",
      "Validation set perplexity: 4.17\n",
      "Average loss at step 5700: 1.614377 learning rate: 1.000000\n",
      "Minibatch perplexity: 4.77\n",
      "Validation set perplexity: 4.14\n",
      "Average loss at step 5800: 1.583890 learning rate: 1.000000\n",
      "Minibatch perplexity: 4.79\n",
      "Validation set perplexity: 4.16\n",
      "Average loss at step 5900: 1.625788 learning rate: 1.000000\n",
      "Minibatch perplexity: 4.50\n",
      "Validation set perplexity: 4.19\n",
      "Average loss at step 6000: 1.585191 learning rate: 1.000000\n",
      "Minibatch perplexity: 4.47\n",
      "================================================================================\n",
      "azi counter caller complet in threat s mount of more of the producer larged in o\n",
      "jts and jears and injuting compater positive a have english certters is opencyce\n",
      "jsdate mals the larging does or done number in his sepers are a s p works now co\n",
      "loted last bappilerwalber two zero zero zero zero the scrash aviding in of one p\n",
      "nu i publically d condepolutionalian telated whiching and for they ii forement e\n",
      "================================================================================\n",
      "Validation set perplexity: 4.16\n",
      "Average loss at step 6100: 1.610149 learning rate: 1.000000\n",
      "Minibatch perplexity: 5.11\n",
      "Validation set perplexity: 4.19\n",
      "Average loss at step 6200: 1.617107 learning rate: 1.000000\n",
      "Minibatch perplexity: 5.16\n",
      "Validation set perplexity: 4.18\n",
      "Average loss at step 6300: 1.632149 learning rate: 1.000000\n",
      "Minibatch perplexity: 5.59\n",
      "Validation set perplexity: 4.16\n",
      "Average loss at step 6400: 1.594682 learning rate: 1.000000\n",
      "Minibatch perplexity: 5.45\n",
      "Validation set perplexity: 4.17\n",
      "Average loss at step 6500: 1.622403 learning rate: 1.000000\n",
      "Minibatch perplexity: 5.75\n",
      "Validation set perplexity: 4.22\n",
      "Average loss at step 6600: 1.583248 learning rate: 1.000000\n",
      "Minibatch perplexity: 5.07\n",
      "Validation set perplexity: 4.21\n",
      "Average loss at step 6700: 1.582497 learning rate: 1.000000\n",
      "Minibatch perplexity: 4.46\n",
      "Validation set perplexity: 4.18\n",
      "Average loss at step 6800: 1.561926 learning rate: 1.000000\n",
      "Minibatch perplexity: 5.47\n",
      "Validation set perplexity: 4.20\n",
      "Average loss at step 6900: 1.612461 learning rate: 1.000000\n",
      "Minibatch perplexity: 5.06\n",
      "Validation set perplexity: 4.22\n",
      "Average loss at step 7000: 1.576213 learning rate: 1.000000\n",
      "Minibatch perplexity: 5.23\n",
      "================================================================================\n",
      "drine on use writh when is new in development croatke so eastin be these southwe\n",
      "qunical nz s was charge turonal of the six five seven five zero six six its c s \n",
      "briam also the barbioge winitical colons dequal ofties since the covered to the \n",
      "i formant sited shugh counters than american in two only that the not chang text\n",
      " king is amerric since abough when beings of mightain of from arel books l own w\n",
      "================================================================================\n",
      "Validation set perplexity: 4.19\n"
     ]
    }
   ],
   "source": [
    "num_steps = 7001\n",
    "summary_frequency = 100 # last 100 batches, to compute mean loss\n",
    "\n",
    "with tf.Session(graph=graph) as session:\n",
    "  tf.global_variables_initializer().run()\n",
    "  print('Initialized')\n",
    "  mean_loss = 0\n",
    "  for step in range(num_steps):\n",
    "    batches = train_batches.next()\n",
    "    feed_dict = dict()\n",
    "    for i in range(num_unrollings + 1):\n",
    "      feed_dict[train_data[i]] = batches[i]\n",
    "    _, l, predictions, lr = session.run(\n",
    "      [optimizer, loss, train_prediction, learning_rate], feed_dict=feed_dict)\n",
    "    mean_loss += l\n",
    "    if step % summary_frequency == 0:\n",
    "      if step > 0:\n",
    "        mean_loss = mean_loss / summary_frequency\n",
    "      # The mean loss is an estimate of the loss over the last few batches.\n",
    "      print(\n",
    "        'Average loss at step %d: %f learning rate: %f' % (step, mean_loss, lr))\n",
    "      mean_loss = 0\n",
    "      labels = np.concatenate(list(batches)[1:]) # (num_unrolls + 1)*batch_size x vocab_size\n",
    "      print('Minibatch perplexity: %.2f' % float(\n",
    "        np.exp(logprob(predictions, labels)))) # perplexity ranges from 1 to a big #\n",
    "      if step % (summary_frequency * 10) == 0:\n",
    "        # Generate some samples.\n",
    "        print('=' * 80) # print = 80 times to generate a border\n",
    "        for _ in range(5):\n",
    "          feed = sample(random_distribution()) # generate a random char in one-hot form\n",
    "          sentence = characters(feed)[0][0] # random char in sentence only\n",
    "          reset_sample_state.run() # reset saved sample output & state to zeros (1 x num_nodes)\n",
    "          for _ in range(79):\n",
    "            prediction = sample_prediction.eval({sample_input: feed}) # feed (dict form) to get prediction of output layer only\n",
    "            # (not LSTM); tensor.eval() must receive a feed_dict\n",
    "            # sample_input: feed, means feed becomes sample_input in the session, goes into lstm_cell, outputs sample_output, \n",
    "            # goes into tf.nn.softmax(tf.nn.xw_plus_b()) to get sample_prediction. It will also save sample output & state\n",
    "            feed = sample(prediction)\n",
    "            sentence += characters(feed)[0][0] #sentence = sample_input, sample_prediction, sample_prediciton, ...\n",
    "          print(sentence)\n",
    "        print('=' * 80)\n",
    "      # Measure validation set perplexity.\n",
    "      reset_sample_state.run()\n",
    "      valid_logprob = 0\n",
    "      for _ in range(valid_size):\n",
    "        b = valid_batches.next() # (num_unrolls + 1 = 2)x(batch_size = 1)x(vocab_size)\n",
    "        predictions = sample_prediction.eval({sample_input: b[0]}) # predict b[1] using b[0]\n",
    "        valid_logprob = valid_logprob + logprob(predictions, b[1]) # compare prediction and b[1], add to accumulator\n",
    "      print('Validation set perplexity: %.2f' % float(np.exp(\n",
    "        valid_logprob / valid_size)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
