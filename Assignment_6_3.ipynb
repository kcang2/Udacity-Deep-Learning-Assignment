{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "from __future__ import print_function\n",
    "import numpy as np\n",
    "import random\n",
    "import os\n",
    "import string\n",
    "import zipfile\n",
    "import tensorflow as tf\n",
    "import sys\n",
    "from six.moves import range\n",
    "from tensorflow.contrib.rnn import LSTMCell, GRUCell\n",
    "import tensorflow.contrib.seq2seq as Seq2Seq"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Found and verified text8.zip\n"
     ]
    }
   ],
   "source": [
    "url = 'http://mattmahoney.net/dc/'\n",
    "\n",
    "def maybe_download(filename, expected_bytes):\n",
    "  \"\"\"Download a file if not present, and make sure it's the right size.\"\"\"\n",
    "  if not os.path.exists(filename):\n",
    "    filename, _ = urlretrieve(url + filename, filename)\n",
    "  statinfo = os.stat(filename)\n",
    "  if statinfo.st_size == expected_bytes:\n",
    "    print('Found and verified %s' % filename)\n",
    "  else:\n",
    "    print(statinfo.st_size)\n",
    "    raise Exception(\n",
    "      'Failed to verify ' + filename + '. Can you get to it with a browser?')\n",
    "  return filename\n",
    "\n",
    "filename = maybe_download('text8.zip', 31344016)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Data size 100000000\n"
     ]
    }
   ],
   "source": [
    "def read_data(filename):\n",
    "  with zipfile.ZipFile(filename) as f:\n",
    "    name = f.namelist()[0]\n",
    "    data = tf.compat.as_str(f.read(name))\n",
    "  return data\n",
    "  \n",
    "text = read_data(filename)\n",
    "print('Data size %d' % len(text))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "99999000 ons anarchists advocate social relations based upon voluntary as\n",
      "1000  anarchism originated as a term of abuse first used against earl\n"
     ]
    }
   ],
   "source": [
    "valid_size = 1000\n",
    "valid_text = text[:valid_size]\n",
    "train_text = text[valid_size:]\n",
    "train_size = len(train_text)\n",
    "print(train_size, train_text[:64])\n",
    "print(valid_size, valid_text[:64])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Unexpected character: ï\n",
      "1 26 27 0\n",
      "a z _\n"
     ]
    }
   ],
   "source": [
    "vocabulary_size = len(string.ascii_lowercase) + 2 # [a-z] + ' ' + '_' (padding)\n",
    "first_letter = ord(string.ascii_lowercase[0])\n",
    "\n",
    "def char2id(char):\n",
    "  if char in string.ascii_lowercase:\n",
    "    return ord(char) - first_letter + 1\n",
    "  elif char == ' ':\n",
    "    return 27\n",
    "  else:\n",
    "    print('Unexpected character: %s' % char)\n",
    "    return 0\n",
    "  \n",
    "def id2char(dictid):\n",
    "  if dictid==27:\n",
    "    return ' '\n",
    "  if dictid > 0:\n",
    "    return chr(dictid + first_letter - 1)\n",
    "  else:\n",
    "    return '_'\n",
    "\n",
    "print(char2id('a'), char2id('z'), char2id(' '), char2id('ï'))\n",
    "print(id2char(1), id2char(26), id2char(0))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 62,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['ons anarchists advocate_______', 'her novels to_________________', 'alc cer do____________________', 'eight zero and________________', 'they were seen________________', 'a combo amplifier_____________', 'a game against________________', 'for the practiced_____________', 'nitrogen dioxide sulfur_______', 'heat engine acts______________', 'way the shoemakers____________', 'zero zloty coin_______________', 'illustration to don___________', 'by his second_________________', 'used credit card______________', 'one two seven_________________', 'drupelets because each________', 'paul jones one________________', 'organization inmarsat interpol', 'people foreign support________', 'the horror writers____________', 'of ireland string_____________', 'connectivity at all___________', 'one day each__________________', 'one p v_______________________', 'by the loss___________________', 'groom at a____________________', 'charles taylor accepted_______', 'nine seven zero_______________', 'throughout most of____________', 'and paralyzes alabama_________', 'to have a_____________________']\n",
      "['msihcrana detanigiro sa_______']\n"
     ]
    }
   ],
   "source": [
    "batch_size=32 # number of sentences (axis=1 of batch array) \n",
    "num_unrollings=3 # number of words\n",
    "maxlen = 30 # 30 characters at most. pad with 0 '_'\n",
    "\n",
    "class BatchGenerator(object):\n",
    "  def __init__(self, text, batch_size, num_unrollings):\n",
    "    self._text = text\n",
    "    self._words = text.split()\n",
    "    self._num_words = len(self._words)\n",
    "    self._batch_size = batch_size\n",
    "    self._num_unrollings = num_unrollings\n",
    "    segment = self._num_words // batch_size # larger than num_unrollings, to prevent getting same phrases in different... \n",
    "    # sentences, however the code does not assert so\n",
    "    self._cursor = [ offset * segment for offset in range(batch_size)] # each batch/sentence has its own cursor, positioned...\n",
    "    # \"segment\" indices away\n",
    "  \n",
    "  def _next_batch(self):\n",
    "    \"\"\"Generate a single batch from the current cursor position in the data.\"\"\"\n",
    "    \n",
    "    batches = np.zeros(shape=[maxlen,self._batch_size],dtype=np.int32)\n",
    "    seqlen = list()\n",
    "    labels = np.zeros(shape=[maxlen,self._batch_size],dtype=np.int32)\n",
    "    for b in range(self._batch_size):\n",
    "      s = list()\n",
    "      l = list()\n",
    "      for x in range(num_unrollings):\n",
    "        w = self._words[self._cursor[b]]  \n",
    "        s.extend([char2id(i) for i in w])\n",
    "        l.extend([char2id(i) for i in reversed(w)])\n",
    "        if x < num_unrollings-1:\n",
    "          s.extend([27])\n",
    "          l.extend([27])\n",
    "        self._cursor[b] = (self._cursor[b] + 1) % self._num_words # move a cursor of a sentence/batch to next char...\n",
    "      seqlen.extend([len(s)])\n",
    "      while len(s)<maxlen:\n",
    "        s.extend([0])\n",
    "        l.extend([0])\n",
    "      if(len(s)>maxlen):\n",
    "        s=s[:maxlen]\n",
    "        l=l[:maxlen]\n",
    "        seqlen[-1]=maxlen\n",
    "      batches[:,b]=s\n",
    "      labels[:,b]=l\n",
    "    return batches,seqlen,labels\n",
    "\n",
    "\n",
    "def characters(probabilities):\n",
    "  \"\"\"Turn a 1-hot encoding or a probability distribution over the possible\n",
    "  characters back into its (most likely) character representation.\"\"\"\n",
    "  return [id2char(c) for c in np.argmax(probabilities, 1)]\n",
    "\n",
    "\n",
    "def batches2string(batches):\n",
    "  #Convert a batch double-list to their string representation.\n",
    "  S = list()\n",
    "  for i in range(batches.shape[1]):\n",
    "    s=''\n",
    "    for j in range(maxlen):\n",
    "      s = s + id2char(batches[j,i])\n",
    "    S.append(s)\n",
    "  return S\n",
    "            \n",
    "def labels2string(labels):\n",
    "  #Convert a batch double-list to their string representation.\n",
    "  S = list()\n",
    "  for i in labels:\n",
    "    s=''\n",
    "    for j in i:\n",
    "      s = s + id2char(j)\n",
    "    S.append(s)\n",
    "  return S\n",
    "\n",
    "train_batches = BatchGenerator(train_text, batch_size, num_unrollings) \n",
    "valid_batches = BatchGenerator(valid_text, 1, num_unrollings)\n",
    "\n",
    "print(batches2string(train_batches._next_batch()[0]))\n",
    "#print(train_batches._next_batch()[1])\n",
    "print(batches2string(valid_batches._next_batch()[2]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 83,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Tensor(\"decoder/TensorArrayStack/TensorArrayGatherV3:0\", shape=(?, 32, 28), dtype=float32)\n",
      "Tensor(\"one_hot:0\", shape=(?, 32, 28), dtype=float32)\n"
     ]
    }
   ],
   "source": [
    "num_nodes = 64\n",
    "embedding_size = 128\n",
    "\n",
    "graph = tf.Graph()\n",
    "with graph.as_default():\n",
    "    \n",
    "  embedding_encoder = tf.Variable(tf.random_uniform([vocabulary_size, embedding_size], -0.1, 0.1,dtype=tf.float32))\n",
    "  encoder_inputs = tf.placeholder(shape=[None,batch_size], dtype=tf.int32)\n",
    "  encoder_lengths=tf.placeholder(dtype=tf.int32,shape=batch_size)\n",
    "  encoder_emb_inp = tf.nn.embedding_lookup(embedding_encoder, encoder_inputs)\n",
    " \n",
    "  encoder_cell = tf.nn.rnn_cell.BasicLSTMCell(num_nodes)\n",
    "  encoder_outputs, encoder_state = tf.nn.dynamic_rnn(encoder_cell, encoder_emb_inp, dtype=tf.float32, time_major=True,\\\n",
    "                                                    sequence_length=encoder_lengths)\n",
    "  del encoder_outputs    \n",
    "  \n",
    "  embedding_decoder = tf.Variable(tf.random_uniform([vocabulary_size, embedding_size], -0.1, 0.1,dtype=tf.float32))\n",
    "  decoder_inputs = tf.placeholder(shape=[None,batch_size], dtype=tf.int32)\n",
    "  decoder_emb_inp = tf.nn.embedding_lookup(embedding_decoder, decoder_inputs)\n",
    "  decoder_cell = tf.nn.rnn_cell.BasicLSTMCell(num_nodes) \n",
    "  decoder_lengths=tf.placeholder(dtype=tf.int32,shape=batch_size)\n",
    "  helper = tf.contrib.seq2seq.TrainingHelper(decoder_emb_inp,sequence_length= decoder_lengths, time_major=True)\n",
    "   \n",
    "  \n",
    "  proj_layer=tf.layers.Dense(vocabulary_size,activation=None,use_bias=False)\n",
    "  decoder = tf.contrib.seq2seq.BasicDecoder(decoder_cell, helper, encoder_state,output_layer=proj_layer)   \n",
    "  decoder_outputs,_,_ = tf.contrib.seq2seq.dynamic_decode(decoder,output_time_major=True)\n",
    "  \n",
    "  decoder_logits = decoder_outputs.rnn_output\n",
    "  decoder_prediction= decoder_outputs.sample_id\n",
    "  print(decoder_logits)\n",
    "  decoder_targets=tf.one_hot(decoder_inputs, depth=vocabulary_size, dtype=tf.float32)\n",
    "  print(decoder_targets)\n",
    "  loss = tf.reduce_mean(tf.nn.softmax_cross_entropy_with_logits_v2(\n",
    "    labels=decoder_targets,logits=decoder_logits))\n",
    " \n",
    "  params = tf.trainable_variables()\n",
    "  gradients = tf.gradients(loss, params)\n",
    "  clipped_gradients, _ = tf.clip_by_global_norm(gradients, 2)\n",
    "\n",
    "  optimizer = tf.train.AdamOptimizer(0.0005)\n",
    "  update_step = optimizer.apply_gradients(zip(clipped_gradients, params)) \n",
    "   "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 84,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Initialized\n"
     ]
    },
    {
     "ename": "InvalidArgumentError",
     "evalue": "logits and labels must be broadcastable: logits_size=[832,28] labels_size=[960,28]\n\t [[Node: softmax_cross_entropy_with_logits = SoftmaxCrossEntropyWithLogits[T=DT_FLOAT, _device=\"/job:localhost/replica:0/task:0/device:CPU:0\"](softmax_cross_entropy_with_logits/Reshape, softmax_cross_entropy_with_logits/Reshape_1)]]\n\nCaused by op 'softmax_cross_entropy_with_logits', defined at:\n  File \"c:\\users\\phill\\anaconda3\\envs\\tensorflow\\lib\\runpy.py\", line 193, in _run_module_as_main\n    \"__main__\", mod_spec)\n  File \"c:\\users\\phill\\anaconda3\\envs\\tensorflow\\lib\\runpy.py\", line 85, in _run_code\n    exec(code, run_globals)\n  File \"c:\\users\\phill\\anaconda3\\envs\\tensorflow\\lib\\site-packages\\ipykernel_launcher.py\", line 16, in <module>\n    app.launch_new_instance()\n  File \"c:\\users\\phill\\anaconda3\\envs\\tensorflow\\lib\\site-packages\\traitlets\\config\\application.py\", line 658, in launch_instance\n    app.start()\n  File \"c:\\users\\phill\\anaconda3\\envs\\tensorflow\\lib\\site-packages\\ipykernel\\kernelapp.py\", line 486, in start\n    self.io_loop.start()\n  File \"c:\\users\\phill\\anaconda3\\envs\\tensorflow\\lib\\site-packages\\tornado\\platform\\asyncio.py\", line 132, in start\n    self.asyncio_loop.run_forever()\n  File \"c:\\users\\phill\\anaconda3\\envs\\tensorflow\\lib\\asyncio\\base_events.py\", line 422, in run_forever\n    self._run_once()\n  File \"c:\\users\\phill\\anaconda3\\envs\\tensorflow\\lib\\asyncio\\base_events.py\", line 1434, in _run_once\n    handle._run()\n  File \"c:\\users\\phill\\anaconda3\\envs\\tensorflow\\lib\\asyncio\\events.py\", line 145, in _run\n    self._callback(*self._args)\n  File \"c:\\users\\phill\\anaconda3\\envs\\tensorflow\\lib\\site-packages\\tornado\\platform\\asyncio.py\", line 122, in _handle_events\n    handler_func(fileobj, events)\n  File \"c:\\users\\phill\\anaconda3\\envs\\tensorflow\\lib\\site-packages\\tornado\\stack_context.py\", line 300, in null_wrapper\n    return fn(*args, **kwargs)\n  File \"c:\\users\\phill\\anaconda3\\envs\\tensorflow\\lib\\site-packages\\zmq\\eventloop\\zmqstream.py\", line 450, in _handle_events\n    self._handle_recv()\n  File \"c:\\users\\phill\\anaconda3\\envs\\tensorflow\\lib\\site-packages\\zmq\\eventloop\\zmqstream.py\", line 480, in _handle_recv\n    self._run_callback(callback, msg)\n  File \"c:\\users\\phill\\anaconda3\\envs\\tensorflow\\lib\\site-packages\\zmq\\eventloop\\zmqstream.py\", line 432, in _run_callback\n    callback(*args, **kwargs)\n  File \"c:\\users\\phill\\anaconda3\\envs\\tensorflow\\lib\\site-packages\\tornado\\stack_context.py\", line 300, in null_wrapper\n    return fn(*args, **kwargs)\n  File \"c:\\users\\phill\\anaconda3\\envs\\tensorflow\\lib\\site-packages\\ipykernel\\kernelbase.py\", line 283, in dispatcher\n    return self.dispatch_shell(stream, msg)\n  File \"c:\\users\\phill\\anaconda3\\envs\\tensorflow\\lib\\site-packages\\ipykernel\\kernelbase.py\", line 233, in dispatch_shell\n    handler(stream, idents, msg)\n  File \"c:\\users\\phill\\anaconda3\\envs\\tensorflow\\lib\\site-packages\\ipykernel\\kernelbase.py\", line 399, in execute_request\n    user_expressions, allow_stdin)\n  File \"c:\\users\\phill\\anaconda3\\envs\\tensorflow\\lib\\site-packages\\ipykernel\\ipkernel.py\", line 208, in do_execute\n    res = shell.run_cell(code, store_history=store_history, silent=silent)\n  File \"c:\\users\\phill\\anaconda3\\envs\\tensorflow\\lib\\site-packages\\ipykernel\\zmqshell.py\", line 537, in run_cell\n    return super(ZMQInteractiveShell, self).run_cell(*args, **kwargs)\n  File \"c:\\users\\phill\\anaconda3\\envs\\tensorflow\\lib\\site-packages\\IPython\\core\\interactiveshell.py\", line 2662, in run_cell\n    raw_cell, store_history, silent, shell_futures)\n  File \"c:\\users\\phill\\anaconda3\\envs\\tensorflow\\lib\\site-packages\\IPython\\core\\interactiveshell.py\", line 2785, in _run_cell\n    interactivity=interactivity, compiler=compiler, result=result)\n  File \"c:\\users\\phill\\anaconda3\\envs\\tensorflow\\lib\\site-packages\\IPython\\core\\interactiveshell.py\", line 2901, in run_ast_nodes\n    if self.run_code(code, result):\n  File \"c:\\users\\phill\\anaconda3\\envs\\tensorflow\\lib\\site-packages\\IPython\\core\\interactiveshell.py\", line 2961, in run_code\n    exec(code_obj, self.user_global_ns, self.user_ns)\n  File \"<ipython-input-83-add5807adaf7>\", line 34, in <module>\n    labels=decoder_targets,logits=decoder_logits))\n  File \"c:\\users\\phill\\anaconda3\\envs\\tensorflow\\lib\\site-packages\\tensorflow\\python\\ops\\nn_ops.py\", line 1879, in softmax_cross_entropy_with_logits_v2\n    precise_logits, labels, name=name)\n  File \"c:\\users\\phill\\anaconda3\\envs\\tensorflow\\lib\\site-packages\\tensorflow\\python\\ops\\gen_nn_ops.py\", line 7738, in softmax_cross_entropy_with_logits\n    name=name)\n  File \"c:\\users\\phill\\anaconda3\\envs\\tensorflow\\lib\\site-packages\\tensorflow\\python\\framework\\op_def_library.py\", line 787, in _apply_op_helper\n    op_def=op_def)\n  File \"c:\\users\\phill\\anaconda3\\envs\\tensorflow\\lib\\site-packages\\tensorflow\\python\\framework\\ops.py\", line 3414, in create_op\n    op_def=op_def)\n  File \"c:\\users\\phill\\anaconda3\\envs\\tensorflow\\lib\\site-packages\\tensorflow\\python\\framework\\ops.py\", line 1740, in __init__\n    self._traceback = self._graph._extract_stack()  # pylint: disable=protected-access\n\nInvalidArgumentError (see above for traceback): logits and labels must be broadcastable: logits_size=[832,28] labels_size=[960,28]\n\t [[Node: softmax_cross_entropy_with_logits = SoftmaxCrossEntropyWithLogits[T=DT_FLOAT, _device=\"/job:localhost/replica:0/task:0/device:CPU:0\"](softmax_cross_entropy_with_logits/Reshape, softmax_cross_entropy_with_logits/Reshape_1)]]\n",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mInvalidArgumentError\u001b[0m                      Traceback (most recent call last)",
      "\u001b[1;32mc:\\users\\phill\\anaconda3\\envs\\tensorflow\\lib\\site-packages\\tensorflow\\python\\client\\session.py\u001b[0m in \u001b[0;36m_do_call\u001b[1;34m(self, fn, *args)\u001b[0m\n\u001b[0;32m   1321\u001b[0m     \u001b[1;32mtry\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m-> 1322\u001b[1;33m       \u001b[1;32mreturn\u001b[0m \u001b[0mfn\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m*\u001b[0m\u001b[0margs\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m   1323\u001b[0m     \u001b[1;32mexcept\u001b[0m \u001b[0merrors\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mOpError\u001b[0m \u001b[1;32mas\u001b[0m \u001b[0me\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mc:\\users\\phill\\anaconda3\\envs\\tensorflow\\lib\\site-packages\\tensorflow\\python\\client\\session.py\u001b[0m in \u001b[0;36m_run_fn\u001b[1;34m(feed_dict, fetch_list, target_list, options, run_metadata)\u001b[0m\n\u001b[0;32m   1306\u001b[0m       return self._call_tf_sessionrun(\n\u001b[1;32m-> 1307\u001b[1;33m           options, feed_dict, fetch_list, target_list, run_metadata)\n\u001b[0m\u001b[0;32m   1308\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mc:\\users\\phill\\anaconda3\\envs\\tensorflow\\lib\\site-packages\\tensorflow\\python\\client\\session.py\u001b[0m in \u001b[0;36m_call_tf_sessionrun\u001b[1;34m(self, options, feed_dict, fetch_list, target_list, run_metadata)\u001b[0m\n\u001b[0;32m   1408\u001b[0m           \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_session\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0moptions\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mfeed_dict\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mfetch_list\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mtarget_list\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m-> 1409\u001b[1;33m           run_metadata)\n\u001b[0m\u001b[0;32m   1410\u001b[0m     \u001b[1;32melse\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;31mInvalidArgumentError\u001b[0m: logits and labels must be broadcastable: logits_size=[832,28] labels_size=[960,28]\n\t [[Node: softmax_cross_entropy_with_logits = SoftmaxCrossEntropyWithLogits[T=DT_FLOAT, _device=\"/job:localhost/replica:0/task:0/device:CPU:0\"](softmax_cross_entropy_with_logits/Reshape, softmax_cross_entropy_with_logits/Reshape_1)]]",
      "\nDuring handling of the above exception, another exception occurred:\n",
      "\u001b[1;31mInvalidArgumentError\u001b[0m                      Traceback (most recent call last)",
      "\u001b[1;32m<ipython-input-84-15d0590d60c1>\u001b[0m in \u001b[0;36m<module>\u001b[1;34m()\u001b[0m\n\u001b[0;32m     14\u001b[0m     \u001b[0mfeed_dict\u001b[0m\u001b[1;33m[\u001b[0m\u001b[0mencoder_lengths\u001b[0m\u001b[1;33m]\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mseqlen\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     15\u001b[0m     \u001b[0mfeed_dict\u001b[0m\u001b[1;33m[\u001b[0m\u001b[0mdecoder_lengths\u001b[0m\u001b[1;33m]\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mseqlen\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m---> 16\u001b[1;33m     \u001b[0m_\u001b[0m\u001b[1;33m,\u001b[0m\u001b[0ml\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mpredictions\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0msession\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mrun\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m[\u001b[0m\u001b[0mupdate_step\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mloss\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mdecoder_prediction\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m,\u001b[0m\u001b[0mfeed_dict\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mfeed_dict\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m     17\u001b[0m     \u001b[0mprint\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mbatches2string\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mprediction\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     18\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mc:\\users\\phill\\anaconda3\\envs\\tensorflow\\lib\\site-packages\\tensorflow\\python\\client\\session.py\u001b[0m in \u001b[0;36mrun\u001b[1;34m(self, fetches, feed_dict, options, run_metadata)\u001b[0m\n\u001b[0;32m    898\u001b[0m     \u001b[1;32mtry\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    899\u001b[0m       result = self._run(None, fetches, feed_dict, options_ptr,\n\u001b[1;32m--> 900\u001b[1;33m                          run_metadata_ptr)\n\u001b[0m\u001b[0;32m    901\u001b[0m       \u001b[1;32mif\u001b[0m \u001b[0mrun_metadata\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    902\u001b[0m         \u001b[0mproto_data\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mtf_session\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mTF_GetBuffer\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mrun_metadata_ptr\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mc:\\users\\phill\\anaconda3\\envs\\tensorflow\\lib\\site-packages\\tensorflow\\python\\client\\session.py\u001b[0m in \u001b[0;36m_run\u001b[1;34m(self, handle, fetches, feed_dict, options, run_metadata)\u001b[0m\n\u001b[0;32m   1133\u001b[0m     \u001b[1;32mif\u001b[0m \u001b[0mfinal_fetches\u001b[0m \u001b[1;32mor\u001b[0m \u001b[0mfinal_targets\u001b[0m \u001b[1;32mor\u001b[0m \u001b[1;33m(\u001b[0m\u001b[0mhandle\u001b[0m \u001b[1;32mand\u001b[0m \u001b[0mfeed_dict_tensor\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1134\u001b[0m       results = self._do_run(handle, final_targets, final_fetches,\n\u001b[1;32m-> 1135\u001b[1;33m                              feed_dict_tensor, options, run_metadata)\n\u001b[0m\u001b[0;32m   1136\u001b[0m     \u001b[1;32melse\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1137\u001b[0m       \u001b[0mresults\u001b[0m \u001b[1;33m=\u001b[0m \u001b[1;33m[\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mc:\\users\\phill\\anaconda3\\envs\\tensorflow\\lib\\site-packages\\tensorflow\\python\\client\\session.py\u001b[0m in \u001b[0;36m_do_run\u001b[1;34m(self, handle, target_list, fetch_list, feed_dict, options, run_metadata)\u001b[0m\n\u001b[0;32m   1314\u001b[0m     \u001b[1;32mif\u001b[0m \u001b[0mhandle\u001b[0m \u001b[1;32mis\u001b[0m \u001b[1;32mNone\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1315\u001b[0m       return self._do_call(_run_fn, feeds, fetches, targets, options,\n\u001b[1;32m-> 1316\u001b[1;33m                            run_metadata)\n\u001b[0m\u001b[0;32m   1317\u001b[0m     \u001b[1;32melse\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1318\u001b[0m       \u001b[1;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_do_call\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0m_prun_fn\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mhandle\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mfeeds\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mfetches\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mc:\\users\\phill\\anaconda3\\envs\\tensorflow\\lib\\site-packages\\tensorflow\\python\\client\\session.py\u001b[0m in \u001b[0;36m_do_call\u001b[1;34m(self, fn, *args)\u001b[0m\n\u001b[0;32m   1333\u001b[0m         \u001b[1;32mexcept\u001b[0m \u001b[0mKeyError\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1334\u001b[0m           \u001b[1;32mpass\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m-> 1335\u001b[1;33m       \u001b[1;32mraise\u001b[0m \u001b[0mtype\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0me\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mnode_def\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mop\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mmessage\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m   1336\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1337\u001b[0m   \u001b[1;32mdef\u001b[0m \u001b[0m_extend_graph\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;31mInvalidArgumentError\u001b[0m: logits and labels must be broadcastable: logits_size=[832,28] labels_size=[960,28]\n\t [[Node: softmax_cross_entropy_with_logits = SoftmaxCrossEntropyWithLogits[T=DT_FLOAT, _device=\"/job:localhost/replica:0/task:0/device:CPU:0\"](softmax_cross_entropy_with_logits/Reshape, softmax_cross_entropy_with_logits/Reshape_1)]]\n\nCaused by op 'softmax_cross_entropy_with_logits', defined at:\n  File \"c:\\users\\phill\\anaconda3\\envs\\tensorflow\\lib\\runpy.py\", line 193, in _run_module_as_main\n    \"__main__\", mod_spec)\n  File \"c:\\users\\phill\\anaconda3\\envs\\tensorflow\\lib\\runpy.py\", line 85, in _run_code\n    exec(code, run_globals)\n  File \"c:\\users\\phill\\anaconda3\\envs\\tensorflow\\lib\\site-packages\\ipykernel_launcher.py\", line 16, in <module>\n    app.launch_new_instance()\n  File \"c:\\users\\phill\\anaconda3\\envs\\tensorflow\\lib\\site-packages\\traitlets\\config\\application.py\", line 658, in launch_instance\n    app.start()\n  File \"c:\\users\\phill\\anaconda3\\envs\\tensorflow\\lib\\site-packages\\ipykernel\\kernelapp.py\", line 486, in start\n    self.io_loop.start()\n  File \"c:\\users\\phill\\anaconda3\\envs\\tensorflow\\lib\\site-packages\\tornado\\platform\\asyncio.py\", line 132, in start\n    self.asyncio_loop.run_forever()\n  File \"c:\\users\\phill\\anaconda3\\envs\\tensorflow\\lib\\asyncio\\base_events.py\", line 422, in run_forever\n    self._run_once()\n  File \"c:\\users\\phill\\anaconda3\\envs\\tensorflow\\lib\\asyncio\\base_events.py\", line 1434, in _run_once\n    handle._run()\n  File \"c:\\users\\phill\\anaconda3\\envs\\tensorflow\\lib\\asyncio\\events.py\", line 145, in _run\n    self._callback(*self._args)\n  File \"c:\\users\\phill\\anaconda3\\envs\\tensorflow\\lib\\site-packages\\tornado\\platform\\asyncio.py\", line 122, in _handle_events\n    handler_func(fileobj, events)\n  File \"c:\\users\\phill\\anaconda3\\envs\\tensorflow\\lib\\site-packages\\tornado\\stack_context.py\", line 300, in null_wrapper\n    return fn(*args, **kwargs)\n  File \"c:\\users\\phill\\anaconda3\\envs\\tensorflow\\lib\\site-packages\\zmq\\eventloop\\zmqstream.py\", line 450, in _handle_events\n    self._handle_recv()\n  File \"c:\\users\\phill\\anaconda3\\envs\\tensorflow\\lib\\site-packages\\zmq\\eventloop\\zmqstream.py\", line 480, in _handle_recv\n    self._run_callback(callback, msg)\n  File \"c:\\users\\phill\\anaconda3\\envs\\tensorflow\\lib\\site-packages\\zmq\\eventloop\\zmqstream.py\", line 432, in _run_callback\n    callback(*args, **kwargs)\n  File \"c:\\users\\phill\\anaconda3\\envs\\tensorflow\\lib\\site-packages\\tornado\\stack_context.py\", line 300, in null_wrapper\n    return fn(*args, **kwargs)\n  File \"c:\\users\\phill\\anaconda3\\envs\\tensorflow\\lib\\site-packages\\ipykernel\\kernelbase.py\", line 283, in dispatcher\n    return self.dispatch_shell(stream, msg)\n  File \"c:\\users\\phill\\anaconda3\\envs\\tensorflow\\lib\\site-packages\\ipykernel\\kernelbase.py\", line 233, in dispatch_shell\n    handler(stream, idents, msg)\n  File \"c:\\users\\phill\\anaconda3\\envs\\tensorflow\\lib\\site-packages\\ipykernel\\kernelbase.py\", line 399, in execute_request\n    user_expressions, allow_stdin)\n  File \"c:\\users\\phill\\anaconda3\\envs\\tensorflow\\lib\\site-packages\\ipykernel\\ipkernel.py\", line 208, in do_execute\n    res = shell.run_cell(code, store_history=store_history, silent=silent)\n  File \"c:\\users\\phill\\anaconda3\\envs\\tensorflow\\lib\\site-packages\\ipykernel\\zmqshell.py\", line 537, in run_cell\n    return super(ZMQInteractiveShell, self).run_cell(*args, **kwargs)\n  File \"c:\\users\\phill\\anaconda3\\envs\\tensorflow\\lib\\site-packages\\IPython\\core\\interactiveshell.py\", line 2662, in run_cell\n    raw_cell, store_history, silent, shell_futures)\n  File \"c:\\users\\phill\\anaconda3\\envs\\tensorflow\\lib\\site-packages\\IPython\\core\\interactiveshell.py\", line 2785, in _run_cell\n    interactivity=interactivity, compiler=compiler, result=result)\n  File \"c:\\users\\phill\\anaconda3\\envs\\tensorflow\\lib\\site-packages\\IPython\\core\\interactiveshell.py\", line 2901, in run_ast_nodes\n    if self.run_code(code, result):\n  File \"c:\\users\\phill\\anaconda3\\envs\\tensorflow\\lib\\site-packages\\IPython\\core\\interactiveshell.py\", line 2961, in run_code\n    exec(code_obj, self.user_global_ns, self.user_ns)\n  File \"<ipython-input-83-add5807adaf7>\", line 34, in <module>\n    labels=decoder_targets,logits=decoder_logits))\n  File \"c:\\users\\phill\\anaconda3\\envs\\tensorflow\\lib\\site-packages\\tensorflow\\python\\ops\\nn_ops.py\", line 1879, in softmax_cross_entropy_with_logits_v2\n    precise_logits, labels, name=name)\n  File \"c:\\users\\phill\\anaconda3\\envs\\tensorflow\\lib\\site-packages\\tensorflow\\python\\ops\\gen_nn_ops.py\", line 7738, in softmax_cross_entropy_with_logits\n    name=name)\n  File \"c:\\users\\phill\\anaconda3\\envs\\tensorflow\\lib\\site-packages\\tensorflow\\python\\framework\\op_def_library.py\", line 787, in _apply_op_helper\n    op_def=op_def)\n  File \"c:\\users\\phill\\anaconda3\\envs\\tensorflow\\lib\\site-packages\\tensorflow\\python\\framework\\ops.py\", line 3414, in create_op\n    op_def=op_def)\n  File \"c:\\users\\phill\\anaconda3\\envs\\tensorflow\\lib\\site-packages\\tensorflow\\python\\framework\\ops.py\", line 1740, in __init__\n    self._traceback = self._graph._extract_stack()  # pylint: disable=protected-access\n\nInvalidArgumentError (see above for traceback): logits and labels must be broadcastable: logits_size=[832,28] labels_size=[960,28]\n\t [[Node: softmax_cross_entropy_with_logits = SoftmaxCrossEntropyWithLogits[T=DT_FLOAT, _device=\"/job:localhost/replica:0/task:0/device:CPU:0\"](softmax_cross_entropy_with_logits/Reshape, softmax_cross_entropy_with_logits/Reshape_1)]]\n"
     ]
    }
   ],
   "source": [
    "num_steps = 7001\n",
    "\n",
    "with tf.Session(graph=graph) as session:\n",
    "  tf.global_variables_initializer().run()\n",
    "  print('Initialized')\n",
    "  \n",
    "  for step in range(num_steps):\n",
    "    batches,seqlen,labels = train_batches._next_batch()\n",
    "    #print(seqlen)\n",
    "    #print(maxlen*batch_size-sum(seqlen))\n",
    "    feed_dict = dict()\n",
    "    feed_dict[encoder_inputs] = batches\n",
    "    feed_dict[decoder_inputs] = labels\n",
    "    feed_dict[encoder_lengths] = seqlen\n",
    "    feed_dict[decoder_lengths] = seqlen\n",
    "    _,l, predictions=session.run([update_step, loss, decoder_prediction],feed_dict=feed_dict)\n",
    "    print(batches2string(prediction))\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
